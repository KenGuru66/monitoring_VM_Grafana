#!/usr/bin/env python3
"""
–ü–ê–†–ê–õ–õ–ï–õ–¨–ù–ê–Ø –≤–µ—Ä—Å–∏—è –∏–º–ø–æ—Ä—Ç–∞ CSV –≤ VictoriaMetrics.

–û—Å–Ω–æ–≤–Ω—ã–µ —É–ª—É—á—à–µ–Ω–∏—è:
- Multiprocessing: –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –≤—Å–µ –¥–æ—Å—Ç—É–ø–Ω—ã–µ CPU —è–¥—Ä–∞
- –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ CSV –Ω–∞ chunks –¥–ª—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏
- –ö–∞–∂–¥—ã–π worker –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —Å–≤–æ–π chunk –Ω–µ–∑–∞–≤–∏—Å–∏–º–æ
- –û–∂–∏–¥–∞–µ–º–æ–µ —É—Å–∫–æ—Ä–µ–Ω–∏–µ: 5-7x –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å csv2vm_streaming.py
"""

import argparse
import csv
import pathlib
import requests
import sys
import logging
import time
from datetime import datetime
from multiprocessing import Pool, cpu_count, Manager
import os

# –ü—ã—Ç–∞–µ–º—Å—è –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å psutil, –Ω–æ —Ä–∞–±–æ—Ç–∞–µ–º –∏ –±–µ–∑ –Ω–µ–≥–æ
try:
    import psutil
    PSUTIL_AVAILABLE = True
except ImportError:
    PSUTIL_AVAILABLE = False

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('csv2vm_parallel.log', mode='a', encoding='utf-8'),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)

# URL endpoint –¥–ª—è –∏–º–ø–æ—Ä—Ç–∞ –¥–∞–Ω–Ω—ã—Ö –≤ VictoriaMetrics
DEFAULT_URL = "http://localhost:8428/api/v1/import/prometheus"

def detect_delimiter(path: pathlib.Path) -> str:
    """–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å CSV —Ñ–∞–π–ª–∞."""
    with open(path, 'r', encoding='utf-8') as f:
        first_line = f.readline()
        if '\t' in first_line:
            return '\t'
        elif ';' in first_line:
            return ';'
        return ','

def sanitize_metric_name(name: str) -> str:
    """–ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç —Å—Ç—Ä–æ–∫—É –≤ –≤–∞–ª–∏–¥–Ω–æ–µ –∏–º—è –º–µ—Ç—Ä–∏–∫–∏ Prometheus."""
    result = name.strip().lower()
    result = result.replace(" (%)", "_pct")
    result = result.replace("(%)", "_pct")
    result = result.replace(" (mbps)", "_mbps")
    result = result.replace(" (iops)", "_iops")
    result = result.replace("(", "").replace(")", "")
    result = result.replace(" ", "_")
    result = result.replace("-", "_")
    result = result.replace("/", "_")
    result = result.replace("%", "percent")
    result = result.replace(".", "")
    result = result.replace(",", "")
    result = result.replace(":", "")
    result = result.replace("[", "").replace("]", "")
    result = result.replace("+", "plus")
    result = result.replace("‚àû", "inf")
    return result

def row_to_prom(row: list, array_sn: str) -> str:
    """–ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç —Å—Ç—Ä–æ–∫—É CSV –≤ —Ñ–æ—Ä–º–∞—Ç Prometheus."""
    if len(row) < 6:
        return None
    
    try:
        resource = row[0].strip()
        metric_name = "hu_" + sanitize_metric_name(row[1]) + "_variable"
        element = row[2].strip()
        value = row[3].strip()
        ts_unix_sec = float(row[5].strip())
        ts_unix_ms = int(ts_unix_sec * 1000)
        
        labels = {
            "Element": element,
            "Resource": resource,
            "SN": array_sn
        }
        
        label_str = ",".join(f'{k}="{v}"' for k, v in sorted(labels.items()))
        return f"{metric_name}{{{label_str}}} {value} {ts_unix_ms}\n"
    except (ValueError, IndexError):
        return None

def send_batch(url: str, batch_lines: list) -> bool:
    """–û—Ç–ø—Ä–∞–≤–ª—è–µ—Ç –±–∞—Ç—á –¥–∞–Ω–Ω—ã—Ö –≤ VictoriaMetrics."""
    if not batch_lines:
        return True
    
    payload = "".join(batch_lines).encode('utf-8')
    
    try:
        r = requests.post(url, data=payload, timeout=60)
        if r.status_code not in (200, 204):
            return False
        return True
    except requests.RequestException:
        return False

def count_lines(file_path: pathlib.Path) -> int:
    """–ë—ã—Å—Ç—Ä–æ –ø–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–æ–∫ –≤ —Ñ–∞–π–ª–µ."""
    with open(file_path, 'rb') as f:
        return sum(1 for _ in f)

def determine_optimal_workers(file_path: pathlib.Path, requested_workers: int = None) -> int:
    """
    –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ worker –ø—Ä–æ—Ü–µ—Å—Å–æ–≤.
    
    –£—á–∏—Ç—ã–≤–∞–µ—Ç:
    - –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ CPU —è–¥–µ—Ä (—Å —Ä–∞–∑—É–º–Ω—ã–º –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º –¥–ª—è –±–æ–ª—å—à–∏—Ö —Å–∏—Å—Ç–µ–º)
    - –î–æ—Å—Ç—É–ø–Ω—É—é –ø–∞–º—è—Ç—å
    - –†–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞
    - –¢–µ–∫—É—â—É—é –∑–∞–≥—Ä—É–∑–∫—É —Å–∏—Å—Ç–µ–º—ã
    
    –ê–¥–∞–ø—Ç–∏—Ä—É–µ—Ç—Å—è –ø–æ–¥ —Å–∏—Å—Ç–µ–º—ã —Å 8, 16, 24, 32+ CPU —è–¥—Ä–∞–º–∏.
    """
    if requested_workers is not None and requested_workers > 0:
        return requested_workers
    
    cpu_cores = cpu_count()
    
    # –ë–∞–∑–æ–≤–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å –∞–¥–∞–ø—Ç–∞—Ü–∏–µ–π –ø–æ–¥ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —è–¥–µ—Ä
    # –î–ª—è –æ—á–µ–Ω—å –±–æ–ª—å—à–∏—Ö —Å–∏—Å—Ç–µ–º –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ä–∞–∑—É–º–Ω–æ–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ
    if cpu_cores <= 8:
        # –ú–∞–ª–µ–Ω—å–∫–∏–µ —Å–∏—Å—Ç–µ–º—ã (–¥–æ 8 CPU): –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ—á—Ç–∏ –≤—Å–µ
        base_workers = max(1, cpu_cores - 1)
    elif cpu_cores <= 16:
        # –°—Ä–µ–¥–Ω–∏–µ —Å–∏—Å—Ç–µ–º—ã (8-16 CPU): –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ—á—Ç–∏ –≤—Å–µ
        base_workers = max(1, cpu_cores - 1)
    elif cpu_cores <= 32:
        # –ë–æ–ª—å—à–∏–µ —Å–∏—Å—Ç–µ–º—ã (16-32 CPU): –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–æ 16 workers
        # –ë–æ–ª—å—à–µ workers —Å–æ–∑–¥–∞—ë—Ç overhead, –∞ VictoriaMetrics –º–æ–∂–µ—Ç –Ω–µ —Å–ø—Ä–∞–≤–∏—Ç—å—Å—è
        base_workers = min(16, cpu_cores - 2)
    else:
        # –û—á–µ–Ω—å –±–æ–ª—å—à–∏–µ —Å–∏—Å—Ç–µ–º—ã (32+ CPU): –º–∞–∫—Å–∏–º—É–º 20 workers
        # Bottleneck –æ–±—ã—á–Ω–æ –≤ I/O –∏ —Å–µ—Ç–∏, –∞ –Ω–µ CPU
        base_workers = 20
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω—É—é –ø–∞–º—è—Ç—å
    mem = psutil.virtual_memory()
    available_gb = mem.available / (1024**3)
    
    # –†–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞
    file_size_gb = file_path.stat().st_size / (1024**3)
    
    # –û—Ü–µ–Ω–∫–∞ –ø–∞–º—è—Ç–∏ –Ω–∞ worker: ~200-300 MB –Ω–∞ worker –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
    memory_per_worker_gb = 0.3
    max_workers_by_memory = int(available_gb / memory_per_worker_gb)
    
    # –î–ª—è –º–∞–ª–µ–Ω—å–∫–∏—Ö —Ñ–∞–π–ª–æ–≤ (<100 MB) –Ω–µ –∏–º–µ–µ—Ç —Å–º—ã—Å–ª–∞ –º–Ω–æ–≥–æ workers
    if file_size_gb < 0.1:
        recommended = min(2, base_workers)
    # –î–ª—è —Å—Ä–µ–¥–Ω–∏—Ö —Ñ–∞–π–ª–æ–≤ (100MB - 1GB)
    elif file_size_gb < 1.0:
        recommended = min(4, base_workers)
    # –î–ª—è –±–æ–ª—å—à–∏—Ö —Ñ–∞–π–ª–æ–≤ (1-5 GB)
    elif file_size_gb < 5.0:
        recommended = base_workers
    # –î–ª—è –æ—á–µ–Ω—å –±–æ–ª—å—à–∏—Ö —Ñ–∞–π–ª–æ–≤ (>5GB)
    else:
        # –î–ª—è –æ–≥—Ä–æ–º–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤ –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≤—Å–µ –¥–æ—Å—Ç—É–ø–Ω—ã–µ workers
        recommended = base_workers
    
    # –£—á–∏—Ç—ã–≤–∞–µ–º —Ç–µ–∫—É—â—É—é –∑–∞–≥—Ä—É–∑–∫—É CPU
    cpu_percent = psutil.cpu_percent(interval=0.1)
    if cpu_percent > 70:
        # –°–∏—Å—Ç–µ–º–∞ —É–∂–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞, —É–º–µ–Ω—å—à–∞–µ–º workers
        recommended = max(1, recommended - 1)
    elif cpu_percent > 50:
        # –°—Ä–µ–¥–Ω—è—è –∑–∞–≥—Ä—É–∑–∫–∞ - –Ω–µ–º–Ω–æ–≥–æ —É–º–µ–Ω—å—à–∞–µ–º
        recommended = max(1, int(recommended * 0.8))
    
    # –§–∏–Ω–∞–ª—å–Ω–æ–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –ø–æ –ø–∞–º—è—Ç–∏
    final_workers = min(recommended, max_workers_by_memory, base_workers)
    
    return max(1, final_workers)

def process_chunk(args):
    """
    –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç chunk CSV —Ñ–∞–π–ª–∞.
    –≠—Ç–∞ —Ñ—É–Ω–∫—Ü–∏—è –∑–∞–ø—É—Å–∫–∞–µ—Ç—Å—è –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø—Ä–æ—Ü–µ—Å—Å–µ.
    """
    file_path, start_line, end_line, url, batch_size, delimiter, array_sn, worker_id = args
    
    worker_logger = logging.getLogger(f"Worker-{worker_id}")
    batch_lines = []
    rows_processed = 0
    lines_converted = 0
    batches_sent = 0
    
    start_time = time.time()
    
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            reader = csv.reader(f, delimiter=delimiter)
            
            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å—Ç—Ä–æ–∫–∏ –¥–æ start_line
            for _ in range(start_line):
                next(reader, None)
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Å—Ç—Ä–æ–∫–∏ –æ—Ç start_line –¥–æ end_line
            for idx in range(end_line - start_line):
                try:
                    row = next(reader)
                except StopIteration:
                    break
                
                if len(row) < 6:
                    continue
                
                # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Å—Ç—Ä–æ–∫—É
                prom_line = row_to_prom(row, array_sn)
                if prom_line:
                    batch_lines.append(prom_line)
                    lines_converted += 1
                
                rows_processed += 1
                
                # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –±–∞—Ç—á –∫–æ–≥–¥–∞ –Ω–∞–∫–æ–ø–∏–ª–∏ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ
                if len(batch_lines) >= batch_size:
                    success = send_batch(url, batch_lines)
                    if success:
                        batches_sent += 1
                        batch_lines = []
                    else:
                        worker_logger.error(f"Failed to send batch {batches_sent + 1}")
                        return {
                            'worker_id': worker_id,
                            'rows_processed': rows_processed,
                            'lines_converted': lines_converted,
                            'batches_sent': batches_sent,
                            'success': False,
                            'time': time.time() - start_time
                        }
            
            # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –æ—Å—Ç–∞—Ç–æ–∫
            if batch_lines:
                success = send_batch(url, batch_lines)
                if success:
                    batches_sent += 1
        
        elapsed = time.time() - start_time
        rate = rows_processed / elapsed if elapsed > 0 else 0
        
        return {
            'worker_id': worker_id,
            'rows_processed': rows_processed,
            'lines_converted': lines_converted,
            'batches_sent': batches_sent,
            'success': True,
            'time': elapsed,
            'rate': rate
        }
    
    except Exception as e:
        worker_logger.error(f"Error in worker {worker_id}: {e}")
        return {
            'worker_id': worker_id,
            'rows_processed': rows_processed,
            'lines_converted': lines_converted,
            'batches_sent': batches_sent,
            'success': False,
            'time': time.time() - start_time
        }

def main_parallel(path: pathlib.Path, url: str, batch_size: int, num_workers: int = None):
    """
    –ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è —Å –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π.
    
    –†–∞–∑–¥–µ–ª—è–µ—Ç CSV –Ω–∞ chunks –∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –∏—Ö –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ.
    """
    logger.info("="*80)
    logger.info(f"CSV2VM PARALLEL Import Started at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    logger.info("="*80)
    logger.info(f"üöÄ Starting PARALLEL import from: {path}")
    logger.info(f"üìç VictoriaMetrics URL: {url}")
    logger.info(f"üì¶ Batch size: {batch_size:,} lines")
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ workers
    optimal_workers = determine_optimal_workers(path, num_workers)
    
    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Å–∏—Å—Ç–µ–º–µ
    mem = psutil.virtual_memory()
    cpu_cores = cpu_count()
    file_size_gb = path.stat().st_size / (1024**3)
    
    logger.info(f"üíª System info:")
    logger.info(f"   CPU cores: {cpu_cores}")
    logger.info(f"   Available memory: {mem.available / (1024**3):.1f} GB / {mem.total / (1024**3):.1f} GB")
    logger.info(f"   File size: {file_size_gb:.2f} GB")
    logger.info(f"   CPU load: {psutil.cpu_percent(interval=0.1):.1f}%")
    
    if num_workers is not None:
        logger.info(f"üë∑ Workers: {optimal_workers} (requested: {num_workers})")
    else:
        logger.info(f"üë∑ Workers: {optimal_workers} (auto-detected, CPU cores: {cpu_cores})")
    
    num_workers = optimal_workers
    array_sn = path.stem
    logger.info(f"üî¢ Array SN: {array_sn}")
    
    delimiter = detect_delimiter(path)
    logger.info(f"üìã Detected delimiter: {'TAB' if delimiter == chr(9) else repr(delimiter)}")
    
    # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º —Å—Ç—Ä–æ–∫–∏ –≤ —Ñ–∞–π–ª–µ
    logger.info("üìä Counting lines in file...")
    total_lines = count_lines(path)
    logger.info(f"   Total lines: {total_lines:,}")
    
    # –†–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ chunks
    lines_per_worker = total_lines // num_workers
    chunks = []
    
    for i in range(num_workers):
        start_line = i * lines_per_worker
        if i == num_workers - 1:
            # –ü–æ—Å–ª–µ–¥–Ω–∏–π worker –±–µ—Ä–µ—Ç –≤—Å–µ –æ—Å—Ç–∞–≤—à–∏–µ—Å—è —Å—Ç—Ä–æ–∫–∏
            end_line = total_lines
        else:
            end_line = (i + 1) * lines_per_worker
        
        chunks.append((
            path, start_line, end_line, url, batch_size, delimiter, array_sn, i + 1
        ))
        logger.info(f"   Worker {i+1}: lines {start_line:,} to {end_line:,} ({end_line - start_line:,} lines)")
    
    # –ó–∞–ø—É—Å–∫–∞–µ–º –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É
    logger.info("üî• Starting parallel processing...")
    start_time = time.time()
    
    with Pool(processes=num_workers) as pool:
        results = pool.map(process_chunk, chunks)
    
    total_time = time.time() - start_time
    
    # –°–æ–±–∏—Ä–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
    total_rows = sum(r['rows_processed'] for r in results)
    total_converted = sum(r['lines_converted'] for r in results)
    total_batches = sum(r['batches_sent'] for r in results)
    all_success = all(r['success'] for r in results)
    avg_rate = total_rows / total_time if total_time > 0 else 0
    
    # –í—ã–≤–æ–¥–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    logger.info("="*80)
    if all_success:
        logger.info("‚úÖ Import completed successfully!")
    else:
        logger.warning("‚ö†Ô∏è Import completed with errors!")
    
    logger.info(f"üìä Statistics:")
    logger.info(f"   - Total rows processed: {total_rows:,}")
    logger.info(f"   - Lines converted: {total_converted:,}")
    logger.info(f"   - Batches sent: {total_batches}")
    logger.info(f"   - Total time: {total_time:.1f} seconds ({total_time/60:.1f} minutes)")
    logger.info(f"   - Average rate: {avg_rate:,.0f} rows/sec")
    logger.info(f"   - Workers used: {num_workers}")
    logger.info(f"   - Array SN: {array_sn}")
    
    logger.info("\nüìà Per-worker performance:")
    for r in results:
        logger.info(f"   Worker {r['worker_id']}: {r['rows_processed']:,} rows in {r['time']:.1f}s "
                   f"({r['rate']:,.0f} rows/sec)")
    
    logger.info("="*80)
    
    print(f"\n{'‚úÖ' if all_success else '‚ö†Ô∏è'} Imported {total_converted:,} rows for SN={array_sn} in {total_time:.1f}s")
    print(f"   Speed: {avg_rate:,.0f} rows/sec (with {num_workers} workers)")
    
    if not all_success:
        sys.exit(1)

if __name__ == "__main__":
    p = argparse.ArgumentParser(
        description="PARALLEL Import OceanStor CSV/TSV to VictoriaMetrics",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
–ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –≤–µ—Ä—Å–∏—è —Å multiprocessing:
- –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –≤—Å–µ –¥–æ—Å—Ç—É–ø–Ω—ã–µ CPU —è–¥—Ä–∞
- –†–∞–∑–¥–µ–ª—è–µ—Ç CSV –Ω–∞ chunks –¥–ª—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏
- –û–∂–∏–¥–∞–µ–º–æ–µ —É—Å–∫–æ—Ä–µ–Ω–∏–µ: 5-7x –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å streaming –≤–µ—Ä—Å–∏–µ–π

–ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:
  %(prog)s 2102353TJWFSP3100020.csv
  %(prog)s 2102353TJWFSP3100020.csv --workers 4
  %(prog)s data.csv --url http://victoriametrics:8428/api/v1/import/prometheus
        """)
    
    p.add_argument("file", type=pathlib.Path, help="CSV/TSV file to import")
    p.add_argument("--url", default=DEFAULT_URL, 
                   help=f"VictoriaMetrics import endpoint (default: {DEFAULT_URL})")
    p.add_argument("--batch", type=int, default=50000, 
                   help="lines per batch request (default: 50000)")
    p.add_argument("-w", "--workers", type=int, default=None,
                   help="Number of worker processes (default: CPU count - 1)")
    
    args = p.parse_args()
    
    if not args.file.exists():
        logger.error(f"‚ùå File not found: {args.file}")
        sys.exit(f"‚ùå File not found: {args.file}")
    
    try:
        main_parallel(args.file, args.url, args.batch, args.workers)
        logger.info(f"Script completed at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    except KeyboardInterrupt:
        logger.warning("‚ö†Ô∏è Import interrupted by user")
        sys.exit(1)
    except Exception as e:
        logger.error(f"‚ùå Fatal error: {e}", exc_info=True)
        raise

