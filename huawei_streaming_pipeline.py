#!/usr/bin/env python3
"""
STREAMING PIPELINE: Huawei Performance Data ‚Üí VictoriaMetrics
–ë–µ–∑ –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã—Ö CSV —Ñ–∞–π–ª–æ–≤ - –ø—Ä—è–º–∞—è –æ—Ç–ø—Ä–∞–≤–∫–∞ –≤ VM.

–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–æ –¥–ª—è –æ–≥—Ä–æ–º–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤ (500GB+, 40+ –º–ª—Ä–¥ —Å—Ç—Ä–æ–∫):
- Streaming –æ–±—Ä–∞–±–æ—Ç–∫–∞ (–≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä—ã)
- –ë–∞—Ç—á–∏–Ω–≥ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏
- –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ .tgz —Ñ–∞–π–ª–æ–≤
- –ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏
- –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ —Ä–µ—Å—É—Ä—Å–æ–≤ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏
"""

import sys
import os
import re
import struct
import tarfile
import zipfile
import time
import argparse
import logging
from pathlib import Path
from datetime import datetime, timedelta
from multiprocessing import Pool, cpu_count, Manager
import requests
from typing import Generator, Tuple
import shutil

# Add parent directory to path
sys.path.insert(0, str(Path(__file__).parent / 'Data2csv'))

try:
    import psutil
    PSUTIL_AVAILABLE = True
except ImportError:
    PSUTIL_AVAILABLE = False
    print("Warning: psutil not available, resource monitoring disabled")

from Data2csv.METRIC_DICT import METRIC_NAME_DICT
from Data2csv.RESOURCE_DICT import RESOURCE_NAME_DICT

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('streaming_pipeline.log', mode='a', encoding='utf-8'),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)

# –ö–æ–Ω—Å—Ç–∞–Ω—Ç—ã
BATCH_SIZE = 100000  # –°—Ç—Ä–æ–∫ –≤ –±–∞—Ç—á–µ –¥–ª—è –æ—Ç–ø—Ä–∞–≤–∫–∏ –≤ VM (–æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–æ)
DEFAULT_RESOURCES = ["207", "212", "225", "216", "266", "10", "11", "21"]
DEFAULT_METRICS = ["18", "22", "25", "28", "23", "26", "1079", "1073", "627", "1074", 
                   "240", "1158", "1154", "1162", "1166", "1170", "1174"]


class ResourceMonitor:
    """–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Ä–µ—Å—É—Ä—Å–æ–≤."""
    
    def __init__(self):
        self.start_time = time.time()
        self.start_memory = None
        self.peak_memory = 0
        self.metrics_sent = 0
        
        if PSUTIL_AVAILABLE:
            process = psutil.Process()
            self.start_memory = process.memory_info().rss / (1024**3)  # GB
    
    def update(self, metrics_count=0):
        """–û–±–Ω–æ–≤–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É."""
        self.metrics_sent += metrics_count
        
        if PSUTIL_AVAILABLE:
            process = psutil.Process()
            current_memory = process.memory_info().rss / (1024**3)
            self.peak_memory = max(self.peak_memory, current_memory)
    
    def report(self):
        """–í—ã–≤–µ—Å—Ç–∏ –æ—Ç—á–µ—Ç."""
        elapsed = time.time() - self.start_time
        
        logger.info("="*80)
        logger.info("üìä RESOURCE USAGE REPORT")
        logger.info("="*80)
        
        if PSUTIL_AVAILABLE:
            process = psutil.Process()
            current_memory = process.memory_info().rss / (1024**3)
            memory_delta = current_memory - self.start_memory if self.start_memory else 0
            
            logger.info(f"üíæ Memory:")
            logger.info(f"   Start:   {self.start_memory:.2f} GB")
            logger.info(f"   Current: {current_memory:.2f} GB")
            logger.info(f"   Peak:    {self.peak_memory:.2f} GB")
            logger.info(f"   Delta:   {memory_delta:+.2f} GB")
            
            logger.info(f"üíª CPU:")
            logger.info(f"   Usage:   {psutil.cpu_percent(interval=0.1):.1f}%")
            logger.info(f"   Cores:   {cpu_count()}")
        
        logger.info(f"üìà Metrics:")
        logger.info(f"   Sent:    {self.metrics_sent:,}")
        logger.info(f"   Rate:    {self.metrics_sent/elapsed:,.0f} metrics/sec")
        
        logger.info(f"‚è±Ô∏è  Time:    {elapsed:.1f} seconds ({elapsed/60:.1f} minutes)")
        logger.info("="*80)


def sanitize_metric_name(name: str) -> str:
    """–ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –Ω–∞–∑–≤–∞–Ω–∏–µ –º–µ—Ç—Ä–∏–∫–∏ –≤ —Ñ–æ—Ä–º–∞—Ç Prometheus."""
    result = name.replace("(%)", "percent").replace(" (%)", "_percent")
    result = result.replace("(", "").replace(")", "")
    result = result.replace("(MB/s)", "mb_s").replace("(KB/s)", "kb_s").replace("(KB)", "kb")
    result = result.replace("(IO/s)", "io_s").replace("(us)", "us").replace("(ms)", "ms")
    result = result.replace("(Bps)", "bps")
    result = result.replace("/", "_").replace("-", "_").replace(".", "").replace(",", "")
    result = result.replace(":", "").replace("[", "").replace("]", "")
    result = result.replace("+‚àû", "inf").replace("+", "plus").replace("‚àû", "inf")
    result = "_".join(result.lower().split())
    while "__" in result:
        result = result.replace("__", "_")
    return result.strip("_")


def construct_data_header(result):
    """–ü–æ—Å—Ç—Ä–æ–∏—Ç—å –∑–∞–≥–æ–ª–æ–≤–æ–∫ –¥–∞–Ω–Ω—ã—Ö."""
    data_header = {}
    if result:
        result = result.groups()
        map_header = result[0]
        map_content = result[1]

        list_map_header = map_header.split(",")
        for each_key in list_map_header:
            list_key_value = each_key.split(":")
            map_key = list_key_value[0].replace('"', '')
            map_value = list_key_value[1].replace('"', '')
            data_header[map_key] = map_value.strip()

        data_header['Map'] = []
        result = re.findall(
            '"([0-9]+)":{"IDs":\\[(("[0-9a-zA-Z]+",?)+)\\],'
            '"Names":\\[(("[.0-9A-Za-z$ \\[\\]\\(\\):_-]*",?)+)\\],'
            '"DataTypes":\\[(([0-9]+,?)+)\\]}',
            map_content
        )
        if result:
            for each_result in result:
                object_type = {}
                object_type['ObjectTypes'] = each_result[0]
                object_type['IDs'] = each_result[1].replace('"', '').split(',')
                object_type['Names'] = each_result[3].replace('"', '').split(',')
                object_type['DataTypes'] = each_result[5].replace('"', '').split(',')
                data_header['Map'].append(object_type)
    return data_header


def construct_data_type(data_header):
    """–ü–æ—Å—Ç—Ä–æ–∏—Ç—å —Å—Ç—Ä—É–∫—Ç—É—Ä—É —Ç–∏–ø–æ–≤ –¥–∞–Ω–Ω—ã—Ö."""
    list_data_type = []
    size_collect_once = 0
    if 'Map' in data_header:
        for resource_type in data_header['Map']:
            size_collect_once += (
                len(resource_type['IDs']) *
                len(resource_type['DataTypes']) * 4
            )
            for index_ids, _ in enumerate(resource_type['IDs']):
                for index_data_type in resource_type['DataTypes']:
                    list_index = [
                        resource_type['ObjectTypes'],
                        index_data_type,
                        resource_type['Names'][index_ids], []
                    ]
                    list_data_type.append(list_index)

    return list_data_type, size_collect_once


def stream_prometheus_metrics(file_path: Path, array_sn: str, resources: list, 
                              metrics: list) -> Generator[str, None, int]:
    """
    STREAMING –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä –º–µ—Ç—Ä–∏–∫ –≤ —Ñ–æ—Ä–º–∞—Ç–µ Prometheus.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç—Ä–æ–∫–∏ –≥–æ—Ç–æ–≤—ã–µ –¥–ª—è –æ—Ç–ø—Ä–∞–≤–∫–∏ –≤ VictoriaMetrics.
    
    Yields:
        str: –ú–µ—Ç—Ä–∏–∫–∞ –≤ —Ñ–æ—Ä–º–∞—Ç–µ Prometheus
    
    Returns:
        int: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫
    """
    metrics_count = 0
    
    try:
        with open(file_path, "rb") as fin:
            # –ß–∏—Ç–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫
            bit_correct = fin.read(32)
            bit_msg_version = fin.read(4)
            bit_equip_sn = fin.read(256).decode('utf-8')
            bit_equip_name = fin.read(41).decode('utf-8')
            bit_equip_data_length = fin.read(4)

            process_finish_flag = False

            bit_map_type = fin.read(4)
            bit_map_length, = struct.unpack("<l", fin.read(4))
            bit_map_value = fin.read(bit_map_length - 8)
            
            if len(bit_map_value) < bit_map_length - 8:
                logger.error(f"Read Data Header Failed for {file_path}")
                return metrics_count

            while not process_finish_flag:
                result = re.match(
                    '{(.*),"Map":{(.*)}}', bit_map_value.decode('utf-8')
                )
                data_header = construct_data_header(result)
                list_data_type, size_collect_once = construct_data_type(data_header)

                times_collect = int(
                    (int(data_header['EndTime']) - int(data_header['StartTime'])) /
                    int(data_header['Archive'])
                )

                # –ß–∏—Ç–∞–µ–º –¥–∞–Ω–Ω—ã–µ –±–ª–æ–∫–∞–º–∏
                for i in range(times_collect):
                    buffer_read = fin.read(size_collect_once)
                    if len(buffer_read) < size_collect_once:
                        process_finish_flag = True
                        break
                    
                    for index_in_buffer in range(0, size_collect_once, 4):
                        bytes_read_4 = buffer_read[index_in_buffer: index_in_buffer + 4]
                        bytes_read_int, = struct.unpack("<l", bytes_read_4)
                        list_data_type[int(index_in_buffer / 4)][3].append(str(bytes_read_int))

                # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º timestamps
                start_time = datetime.fromtimestamp(int(data_header['StartTime']))
                next_time = start_time
                time_list = []
                for i, _ in enumerate(list_data_type[0][3]):
                    time_list.append(next_time)
                    next_time += timedelta(seconds=int(data_header['Archive']))

                # STREAMING: –æ—Ç–¥–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏ –ø–æ –æ–¥–Ω–æ–π, –Ω–µ –Ω–∞–∫–∞–ø–ª–∏–≤–∞—è –≤ –ø–∞–º—è—Ç–∏
                for data_type in list_data_type:
                    # –§–∏–ª—å—Ç—Ä—É–µ–º –Ω—É–∂–Ω—ã–µ —Ä–µ—Å—É—Ä—Å—ã –∏ –º–µ—Ç—Ä–∏–∫–∏
                    if str(data_type[0]) not in resources or str(data_type[1]) not in metrics:
                        continue

                    resource_name = RESOURCE_NAME_DICT.get(str(data_type[0]), f"UNKNOWN_RESOURCE_{data_type[0]}")
                    metric_name = "huawei_" + sanitize_metric_name(
                        METRIC_NAME_DICT.get(str(data_type[1]), f"UNKNOWN_METRIC_{data_type[1]}")
                    )
                    element = data_type[2]

                    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ—Ä–≤–∞–ª–∞
                    for index, point_value in enumerate(data_type[3]):
                        try:
                            ts_unix_ms = int(time.mktime(time_list[index].timetuple()) * 1000)
                            
                            # –§–æ—Ä–º–∞—Ç Prometheus
                            prom_line = f'{metric_name}{{Element="{element}",Resource="{resource_name}",SN="{array_sn}"}} {point_value} {ts_unix_ms}\n'
                            
                            yield prom_line
                            metrics_count += 1
                            
                        except (ValueError, IndexError) as e:
                            continue

                # –û—á–∏—â–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ—Å–≤–æ–±–æ–∂–¥–µ–Ω–∏—è –ø–∞–º—è—Ç–∏
                for data_type in list_data_type:
                    data_type[3].clear()

                bit_map_type = fin.read(4)
                if bit_map_type == b'':
                    process_finish_flag = True
                elif bit_map_type == b'\x00\x00\x00\x00':
                    bit_map_length, = struct.unpack("<l", fin.read(4))
                    if bit_map_length < 8:
                        process_finish_flag = True
                    else:
                        bit_map_value = fin.read(bit_map_length - 8)
                        if len(bit_map_value) < bit_map_length - 8:
                            return metrics_count
                else:
                    process_finish_flag = True
                    
    except Exception as exc_info:
        logger.error(f"Error processing {file_path}: {exc_info}")
    
    return metrics_count


def send_batch_to_vm(batch: list, vm_url: str) -> bool:
    """–û—Ç–ø—Ä–∞–≤–∏—Ç—å –±–∞—Ç—á –º–µ—Ç—Ä–∏–∫ –≤ VictoriaMetrics."""
    if not batch:
        return True
    
    payload = "".join(batch).encode('utf-8')
    
    try:
        response = requests.post(vm_url, data=payload, timeout=30)
        if response.status_code not in (200, 204):
            logger.error(f"VM returned {response.status_code}: {response.text[:200]}")
            return False
        return True
    except requests.RequestException as e:
        logger.error(f"Failed to send batch to VM: {e}")
        return False


def decompress_tgz(file_tgz: Path) -> Path:
    """–†–∞—Å–ø–∞–∫–æ–≤–∞—Ç—å .tgz —Ñ–∞–π–ª."""
    tar = tarfile.open(file_tgz)
    names = tar.getnames()
    temp_file_path = Path("temp") / f"temp_{os.getpid()}_{time.time()}"
    temp_file_path.mkdir(parents=True, exist_ok=True)
    
    if len(names) == 1:
        tar.extract(names[0], temp_file_path)
        return temp_file_path / names[0]
    
    logger.error(f"perf file content error: {file_tgz}")
    return None


def process_single_tgz_streaming(args) -> dict:
    """
    –û–±—Ä–∞–±–æ—Ç–∞—Ç—å –æ–¥–∏–Ω .tgz —Ñ–∞–π–ª –≤ streaming —Ä–µ–∂–∏–º–µ.
    –ü–∞—Ä—Å–∏—Ç –¥–∞–Ω–Ω—ã–µ –∏ —Å—Ä–∞–∑—É –æ—Ç–ø—Ä–∞–≤–ª—è–µ—Ç –≤ VictoriaMetrics –±–∞—Ç—á–∞–º–∏.
    """
    tgz_file, vm_url, batch_size, resources, metrics, array_sn = args
    
    worker_id = os.getpid()
    logger.info(f"[Worker {worker_id}] Processing {tgz_file.name}")
    
    start_time = time.time()
    metrics_sent = 0
    batches_sent = 0
    
    try:
        # –†–∞—Å–ø–∞–∫–æ–≤—ã–≤–∞–µ–º
        decompressed_file = decompress_tgz(tgz_file)
        if not decompressed_file:
            return {
                'file': tgz_file.name,
                'success': False,
                'metrics': 0,
                'time': time.time() - start_time
            }
        
        # –°—Ç—Ä–∏–º–∏–º –º–µ—Ç—Ä–∏–∫–∏ –∏ –æ—Ç–ø—Ä–∞–≤–ª—è–µ–º –±–∞—Ç—á–∞–º–∏
        batch = []
        
        for metric_line in stream_prometheus_metrics(decompressed_file, array_sn, resources, metrics):
            batch.append(metric_line)
            
            # –ö–æ–≥–¥–∞ –±–∞—Ç—á –∑–∞–ø–æ–ª–Ω–µ–Ω - –æ—Ç–ø—Ä–∞–≤–ª—è–µ–º
            if len(batch) >= batch_size:
                if send_batch_to_vm(batch, vm_url):
                    metrics_sent += len(batch)
                    batches_sent += 1
                    batch = []
                else:
                    logger.error(f"[Worker {worker_id}] Failed to send batch")
                    return {
                        'file': tgz_file.name,
                        'success': False,
                        'metrics': metrics_sent,
                        'time': time.time() - start_time
                    }
        
        # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –æ—Å—Ç–∞—Ç–æ–∫
        if batch:
            if send_batch_to_vm(batch, vm_url):
                metrics_sent += len(batch)
                batches_sent += 1
        
        # Cleanup
        if decompressed_file.exists():
            decompressed_file.unlink()
        
        elapsed = time.time() - start_time
        rate = metrics_sent / elapsed if elapsed > 0 else 0
        
        logger.info(f"[Worker {worker_id}] ‚úÖ {tgz_file.name}: {metrics_sent:,} metrics in {elapsed:.1f}s ({rate:,.0f} m/s)")
        
        return {
            'file': tgz_file.name,
            'success': True,
            'metrics': metrics_sent,
            'batches': batches_sent,
            'time': elapsed,
            'rate': rate
        }
        
    except Exception as e:
        logger.error(f"[Worker {worker_id}] Error: {e}")
        return {
            'file': tgz_file.name,
            'success': False,
            'metrics': 0,
            'time': time.time() - start_time
        }


def extract_serial_from_filename(filename: str) -> str:
    """–ò–∑–≤–ª–µ—á—å —Å–µ—Ä–∏–π–Ω—ã–π –Ω–æ–º–µ—Ä –∏–∑ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞."""
    match = re.search(r"_SN_([0-9A-Z]+)_SP\d+", filename)
    if match:
        return match.group(1)
    
    # Fallback: –ø–æ–ø—ã—Ç–∞—Ç—å—Å—è –∏–∑–≤–ª–µ—á—å –∏–∑ –∏–º–µ–Ω–∏ –∞—Ä—Ö–∏–≤–∞
    match = re.search(r"\(([0-9.]+)\)", filename)
    if match:
        return match.group(1).replace(".", "_")
    
    return "UNKNOWN_SN"


def main():
    parser = argparse.ArgumentParser(
        description="STREAMING Pipeline: Huawei Performance ‚Üí VictoriaMetrics (–ë–ï–ó –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã—Ö CSV)",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–æ –¥–ª—è –æ–≥—Ä–æ–º–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤ (500GB+):
  
  ‚Ä¢ Streaming –æ–±—Ä–∞–±–æ—Ç–∫–∞ - –º–∏–Ω–∏–º—É–º –ø–∞–º—è—Ç–∏
  ‚Ä¢ –ü—Ä—è–º–∞—è –æ—Ç–ø—Ä–∞–≤–∫–∞ –≤ VictoriaMetrics
  ‚Ä¢ –ë–ï–ó –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã—Ö CSV —Ñ–∞–π–ª–æ–≤
  ‚Ä¢ –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ .tgz —Ñ–∞–π–ª–æ–≤
  ‚Ä¢ –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ —Ä–µ—Å—É—Ä—Å–æ–≤

–ü—Ä–∏–º–µ—Ä—ã:

  # –ë–∞–∑–æ–≤—ã–π –∑–∞–ø—É—Å–∫
  %(prog)s -i logs.zip
  
  # –° –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–æ–º
  %(prog)s -i logs.zip --monitor
  
  # –ö–∞—Å—Ç–æ–º–Ω—ã–µ —Ä–µ—Å—É—Ä—Å—ã
  %(prog)s -i logs.zip --vm-url http://vm:8428/api/v1/import/prometheus
        """)
    
    parser.add_argument('-i', '--input', type=str, required=True,
                       help='ZIP –∞—Ä—Ö–∏–≤ —Å .tgz —Ñ–∞–π–ª–∞–º–∏')
    parser.add_argument('--vm-url', type=str, 
                       default='http://localhost:8428/api/v1/import/prometheus',
                       help='VictoriaMetrics import endpoint')
    parser.add_argument('--batch-size', type=int, default=BATCH_SIZE,
                       help=f'–†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ (default: {BATCH_SIZE})')
    parser.add_argument('-w', '--workers', type=int, default=None,
                       help='–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö workers (default: CPU-2)')
    parser.add_argument('--all-metrics', action='store_true', default=True,
                       help='–ü–∞—Ä—Å–∏—Ç—å –í–°–ï –º–µ—Ç—Ä–∏–∫–∏ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: True)')
    parser.add_argument('--monitor', action='store_true',
                       help='–í–∫–ª—é—á–∏—Ç—å –ø–æ–¥—Ä–æ–±–Ω—ã–π –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ —Ä–µ—Å—É—Ä—Å–æ–≤')
    
    args = parser.parse_args()
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è
    input_path = Path(args.input)
    if not input_path.exists():
        logger.error(f"File not found: {input_path}")
        sys.exit(1)
    
    monitor = ResourceMonitor() if args.monitor else None
    
    logger.info("="*80)
    logger.info("üöÄ STREAMING PIPELINE STARTED")
    logger.info("="*80)
    logger.info(f"Input:  {input_path}")
    logger.info(f"VM URL: {args.vm_url}")
    logger.info(f"Batch:  {args.batch_size:,} metrics")
    
    if args.all_metrics:
        resources = list(RESOURCE_NAME_DICT.keys())
        metrics = list(METRIC_NAME_DICT.keys())
        logger.info(f"Mode:   ALL METRICS ({len(metrics)} metrics, {len(resources)} resources)")
    else:
        resources = DEFAULT_RESOURCES
        metrics = DEFAULT_METRICS
        logger.info(f"Mode:   DEFAULT ({len(metrics)} metrics, {len(resources)} resources)")
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º workers
    num_workers = args.workers if args.workers else max(1, cpu_count() - 2)
    logger.info(f"Workers: {num_workers}")
    logger.info("="*80)
    
    start_time = time.time()
    
    # –ò–∑–≤–ª–µ–∫–∞–µ–º ZIP
    temp_dir = Path("temp_streaming_extract")
    if temp_dir.exists():
        shutil.rmtree(temp_dir)
    temp_dir.mkdir()
    
    logger.info(f"üì¶ Extracting ZIP...")
    with zipfile.ZipFile(input_path, 'r') as zip_ref:
        zip_ref.extractall(temp_dir)
    
    # –ù–∞—Ö–æ–¥–∏–º .tgz —Ñ–∞–π–ª—ã
    tgz_files = list(temp_dir.rglob("*.tgz"))
    logger.info(f"‚úÖ Found {len(tgz_files)} .tgz files")
    
    if not tgz_files:
        logger.error("No .tgz files found!")
        sys.exit(1)
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Å–µ—Ä–∏–π–Ω—ã–π –Ω–æ–º–µ—Ä
    array_sn = extract_serial_from_filename(tgz_files[0].name)
    logger.info(f"üìå Array SN: {array_sn}")
    logger.info("="*80)
    
    # –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞
    process_args = [
        (f, args.vm_url, args.batch_size, resources, metrics, array_sn)
        for f in tgz_files
    ]
    
    logger.info(f"üî• Processing {len(tgz_files)} files with {num_workers} workers...")
    
    with Pool(processes=num_workers) as pool:
        results = pool.map(process_single_tgz_streaming, process_args)
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    total_time = time.time() - start_time
    total_metrics = sum(r['metrics'] for r in results)
    total_batches = sum(r.get('batches', 0) for r in results)
    success_count = sum(1 for r in results if r['success'])
    
    if monitor:
        monitor.update(total_metrics)
        monitor.report()
    
    logger.info("="*80)
    logger.info("‚úÖ STREAMING PIPELINE COMPLETED")
    logger.info("="*80)
    logger.info(f"üìä Results:")
    logger.info(f"   Files processed: {success_count}/{len(tgz_files)}")
    logger.info(f"   Metrics sent:    {total_metrics:,}")
    logger.info(f"   Batches sent:    {total_batches:,}")
    logger.info(f"   Total time:      {total_time:.1f}s ({total_time/60:.1f} min)")
    logger.info(f"   Throughput:      {total_metrics/total_time:,.0f} metrics/sec")
    logger.info(f"   Array SN:        {array_sn}")
    logger.info("="*80)
    
    # Cleanup
    if temp_dir.exists():
        shutil.rmtree(temp_dir)
    
    temp_path = Path("temp")
    if temp_path.exists():
        for item in temp_path.iterdir():
            if item.is_dir():
                try:
                    shutil.rmtree(item)
                except:
                    pass
    
    print(f"\n‚úÖ Done! Sent {total_metrics:,} metrics in {total_time:.1f}s")
    print(f"üìä Check VictoriaMetrics: {args.vm_url.replace('/api/v1/import/prometheus', '')}")


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        logger.warning("\n‚ö†Ô∏è  Interrupted by user")
        sys.exit(130)
    except Exception as e:
        logger.error(f"\n‚ùå Fatal error: {e}", exc_info=True)
        sys.exit(1)

