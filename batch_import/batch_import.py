#!/usr/bin/env python3
"""
Batch Import Script –¥–ª—è –º–∞—Å—Å–æ–≤–æ–≥–æ –∏–º–ø–æ—Ä—Ç–∞ Huawei Performance –ª–æ–≥–æ–≤ –≤ VictoriaMetrics

–°–∫—Ä–∏–ø—Ç –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç ZIP –∞—Ä—Ö–∏–≤—ã —Å performance –¥–∞–Ω–Ω—ã–º–∏,
–∑–∞–ø—É—Å–∫–∞–µ—Ç streaming pipeline –¥–ª—è –∏–º–ø–æ—Ä—Ç–∞ –≤ VictoriaMetrics –∏ –ø—Ä–æ–≤–µ—Ä—è–µ—Ç
—É—Å–ø–µ—à–Ω–æ—Å—Ç—å –∏–º–ø–æ—Ä—Ç–∞.

–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:
    python3 batch_import.py /path/to/logs/
    python3 batch_import.py /path/to/logs/ --skip-existing
    python3 batch_import.py /path/to/logs/ --dry-run
"""

import sys
import os
import re
import zipfile
import subprocess
import argparse
import logging
import time
import signal
from pathlib import Path
from datetime import datetime, timedelta
from typing import Optional, Tuple, List, Dict, Any
from dataclasses import dataclass, field

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ VictoriaMetricsClient –∏–∑ —Å–æ—Å–µ–¥–Ω–µ–≥–æ –ø—Ä–æ–µ–∫—Ç–∞
sys.path.insert(0, '/data/projects/Huawei_health_check')

try:
    from victoriametrics_client import VictoriaMetricsClient
    VM_CLIENT_AVAILABLE = True
except ImportError:
    VM_CLIENT_AVAILABLE = False
    print("Warning: VictoriaMetricsClient not available, verification will be skipped")


@dataclass
class ImportResult:
    """–†–µ–∑—É–ª—å—Ç–∞—Ç –∏–º–ø–æ—Ä—Ç–∞ –æ–¥–Ω–æ–≥–æ –∞—Ä—Ö–∏–≤–∞."""
    archive_name: str
    serial_number: Optional[str] = None
    status: str = "pending"  # pending, success, failed, skipped
    import_time: float = 0.0
    error_message: Optional[str] = None
    data_in_vm: bool = False
    last_datapoint: Optional[str] = None
    metrics_sent: int = 0
    
    
@dataclass
class BatchStats:
    """–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ batch –∏–º–ø–æ—Ä—Ç–∞."""
    total_archives: int = 0
    processed: int = 0
    successful: int = 0
    failed: int = 0
    skipped: int = 0
    total_time: float = 0.0
    results: List[ImportResult] = field(default_factory=list)
    

# –ì–ª–æ–±–∞–ª—å–Ω–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è –¥–ª—è graceful shutdown
INTERRUPTED = False


def signal_handler(signum, frame):
    """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ —Å–∏–≥–Ω–∞–ª–∞ –ø—Ä–µ—Ä—ã–≤–∞–Ω–∏—è (Ctrl+C)."""
    global INTERRUPTED
    INTERRUPTED = True
    logger.warning("\n‚ö†Ô∏è  –ü—Ä–µ—Ä—ã–≤–∞–Ω–∏–µ –ø–æ–ª—É—á–µ–Ω–æ! –ó–∞–≤–µ—Ä—à–∞—é —Ç–µ–∫—É—â—É—é –æ–ø–µ—Ä–∞—Ü–∏—é...")


def setup_logging(log_dir: Path = Path(".")) -> Tuple[logging.Logger, str]:
    """
    –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è.
    
    Args:
        log_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è –ª–æ–≥-—Ñ–∞–π–ª–∞
        
    Returns:
        Tuple[Logger, –∏–º—è –ª–æ–≥-—Ñ–∞–π–ª–∞]
    """
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_filename = f"batch_import_{timestamp}.log"
    log_path = log_dir / log_filename
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
    log_format = '%(asctime)s - %(levelname)s - %(message)s'
    
    # –°–æ–∑–¥–∞–µ–º logger
    logger = logging.getLogger('batch_import')
    logger.setLevel(logging.DEBUG)
    
    # File handler
    file_handler = logging.FileHandler(log_path, mode='w', encoding='utf-8')
    file_handler.setLevel(logging.DEBUG)
    file_handler.setFormatter(logging.Formatter(log_format))
    
    # Console handler
    console_handler = logging.StreamHandler(sys.stdout)
    console_handler.setLevel(logging.INFO)
    console_handler.setFormatter(logging.Formatter(log_format))
    
    # –î–æ–±–∞–≤–ª—è–µ–º handlers
    logger.addHandler(file_handler)
    logger.addHandler(console_handler)
    
    return logger, log_filename


def extract_serial_number(zip_path: Path) -> Optional[str]:
    """
    –ò–∑–≤–ª–µ–∫–∞–µ—Ç —Å–µ—Ä–∏–π–Ω—ã–π –Ω–æ–º–µ—Ä –º–∞—Å—Å–∏–≤–∞ –∏–∑ –∏–º–µ–Ω–∏ .tgz —Ñ–∞–π–ª–æ–≤ –≤–Ω—É—Ç—Ä–∏ ZIP –∞—Ä—Ö–∏–≤–∞.
    
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç zipfile –¥–ª—è –ø—Ä–æ—Å–º–æ—Ç—Ä–∞ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ –±–µ–∑ –ø–æ–ª–Ω–æ–π —Ä–∞—Å–ø–∞–∫–æ–≤–∫–∏.
    –ò—â–µ—Ç –ø–∞—Ç—Ç–µ—Ä–Ω: PerfData_*_SN_<SERIAL>_SP*
    
    Args:
        zip_path: –ü—É—Ç—å –∫ ZIP –∞—Ä—Ö–∏–≤—É
        
    Returns:
        –°–µ—Ä–∏–π–Ω—ã–π –Ω–æ–º–µ—Ä –∏–ª–∏ None –µ—Å–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω
    """
    try:
        with zipfile.ZipFile(zip_path, 'r') as zip_ref:
            # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ —Ñ–∞–π–ª–æ–≤ –≤ –∞—Ä—Ö–∏–≤–µ
            file_list = zip_ref.namelist()
            
            # –ò—â–µ–º .tgz —Ñ–∞–π–ª—ã —Å –ø–∞—Ç—Ç–µ—Ä–Ω–æ–º SN
            pattern = r"PerfData_.*_SN_([0-9A-Z]+)_SP"
            
            for filename in file_list:
                if filename.endswith('.tgz'):
                    match = re.search(pattern, filename)
                    if match:
                        return match.group(1)
        
        # Fallback: –ø–æ–ø—Ä–æ–±—É–µ–º –∏–∑–≤–ª–µ—á—å –∏–∑ –∏–º–µ–Ω–∏ –∞—Ä—Ö–∏–≤–∞ (–µ—Å–ª–∏ –µ—Å—Ç—å –ø–∞—Ç—Ç–µ—Ä–Ω)
        match = re.search(r"\(([0-9.]+)\)", zip_path.name)
        if match:
            return match.group(1).replace(".", "_")
        
        return None
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ SN –∏–∑ {zip_path.name}: {e}")
        return None


def run_streaming_pipeline(zip_path: Path, vm_url: str, logger: logging.Logger) -> Tuple[bool, str, int]:
    """
    –ó–∞–ø—É—Å–∫–∞–µ—Ç huawei_streaming_pipeline.py —á–µ—Ä–µ–∑ subprocess.
    
    Args:
        zip_path: –ü—É—Ç—å –∫ ZIP –∞—Ä—Ö–∏–≤—É
        vm_url: URL VictoriaMetrics
        logger: Logger –¥–ª—è –≤—ã–≤–æ–¥–∞
        
    Returns:
        Tuple[—É—Å–ø–µ—Ö, –ª–æ–≥–∏, –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –º–µ—Ç—Ä–∏–∫]
    """
    # –ü—É—Ç—å –∫ streaming pipeline (–≤ —Ä–æ–¥–∏—Ç–µ–ª—å—Å–∫–æ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏)
    pipeline_script = Path(__file__).parent.parent / "huawei_streaming_pipeline.py"
    
    if not pipeline_script.exists():
        logger.error(f"–ù–µ –Ω–∞–π–¥–µ–Ω —Å–∫—Ä–∏–ø—Ç: {pipeline_script}")
        return False, "Pipeline script not found", 0
    
    # –§–æ—Ä–º–∏—Ä—É–µ–º –∫–æ–º–∞–Ω–¥—É
    cmd = [
        sys.executable,
        str(pipeline_script),
        "-i", str(zip_path),
        "--vm-url", f"{vm_url}/api/v1/import/prometheus",
        "--monitor"
    ]
    
    logger.info(f"–ó–∞–ø—É—Å–∫: {' '.join(cmd)}")
    
    try:
        # –ó–∞–ø—É—Å–∫–∞–µ–º –ø—Ä–æ—Ü–µ—Å—Å —Å –ø–µ—Ä–µ—Ö–≤–∞—Ç–æ–º –≤—ã–≤–æ–¥–∞
        process = subprocess.Popen(
            cmd,
            stdout=subprocess.PIPE,
            stderr=subprocess.STDOUT,
            universal_newlines=True,
            bufsize=1
        )
        
        output_lines = []
        metrics_sent = 0
        
        # –ß–∏—Ç–∞–µ–º –≤—ã–≤–æ–¥ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏
        for line in process.stdout:
            output_lines.append(line)
            
            # –õ–æ–≥–∏—Ä—É–µ–º –≤–∞–∂–Ω—ã–µ —Å—Ç—Ä–æ–∫–∏
            if "Processing" in line or "‚úÖ" in line or "ERROR" in line or "WARNING" in line:
                logger.debug(line.strip())
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫
            if "Metrics sent:" in line:
                match = re.search(r"Metrics sent:\s+([\d,]+)", line)
                if match:
                    metrics_sent = int(match.group(1).replace(",", ""))
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø—Ä–µ—Ä—ã–≤–∞–Ω–∏–µ
            if INTERRUPTED:
                process.terminate()
                process.wait(timeout=5)
                return False, "Interrupted by user", 0
        
        # –ñ–¥–µ–º –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è
        return_code = process.wait()
        
        full_output = "".join(output_lines)
        
        if return_code == 0:
            logger.info(f"‚úÖ Pipeline –∑–∞–≤–µ—Ä—à–∏–ª—Å—è —É—Å–ø–µ—à–Ω–æ")
            return True, full_output, metrics_sent
        else:
            logger.error(f"‚ùå Pipeline –∑–∞–≤–µ—Ä—à–∏–ª—Å—è —Å –æ—à–∏–±–∫–æ–π (–∫–æ–¥: {return_code})")
            return False, full_output, metrics_sent
            
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø—É—Å–∫–µ pipeline: {e}")
        return False, str(e), 0


def verify_data_in_vm(client: VictoriaMetricsClient, sn: str, logger: logging.Logger) -> Tuple[bool, Optional[str]]:
    """
    –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –Ω–∞–ª–∏—á–∏–µ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–µ—Ä–∏–π–Ω–æ–≥–æ –Ω–æ–º–µ—Ä–∞ –≤ VictoriaMetrics.
    
    Args:
        client: VictoriaMetricsClient
        sn: –°–µ—Ä–∏–π–Ω—ã–π –Ω–æ–º–µ—Ä –º–∞—Å—Å–∏–≤–∞
        logger: Logger –¥–ª—è –≤—ã–≤–æ–¥–∞
        
    Returns:
        Tuple[–¥–∞–Ω–Ω—ã–µ –µ—Å—Ç—å, –¥–∞—Ç–∞ –ø–æ—Å–ª–µ–¥–Ω–µ–π —Ç–æ—á–∫–∏]
    """
    try:
        last_timestamp = client.get_last_datapoint_time(sn)
        
        if last_timestamp:
            last_date = datetime.fromtimestamp(last_timestamp).strftime('%Y-%m-%d %H:%M:%S')
            logger.info(f"‚úÖ –î–∞–Ω–Ω—ã–µ –≤ VM –Ω–∞–π–¥–µ–Ω—ã. –ü–æ—Å–ª–µ–¥–Ω—è—è —Ç–æ—á–∫–∞: {last_date}")
            return True, last_date
        else:
            logger.warning(f"‚ö†Ô∏è  –î–∞–Ω–Ω—ã–µ –¥–ª—è SN {sn} –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –≤ VictoriaMetrics")
            return False, None
            
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ –¥–∞–Ω–Ω—ã—Ö –≤ VM: {e}")
        return False, None


def process_archive(
    zip_path: Path,
    vm_url: str,
    vm_client: Optional[VictoriaMetricsClient],
    skip_existing: bool,
    dry_run: bool,
    logger: logging.Logger
) -> ImportResult:
    """
    –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ–¥–Ω–æ–≥–æ –∞—Ä—Ö–∏–≤–∞.
    
    Args:
        zip_path: –ü—É—Ç—å –∫ ZIP –∞—Ä—Ö–∏–≤—É
        vm_url: URL VictoriaMetrics
        vm_client: VictoriaMetricsClient –∏–ª–∏ None
        skip_existing: –ü—Ä–æ–ø—É—Å–∫–∞—Ç—å –µ—Å–ª–∏ –¥–∞–Ω–Ω—ã–µ —É–∂–µ –µ—Å—Ç—å –≤ VM
        dry_run: –†–µ–∂–∏–º –±–µ–∑ —Ä–µ–∞–ª—å–Ω–æ–≥–æ –∏–º–ø–æ—Ä—Ç–∞
        logger: Logger
        
    Returns:
        ImportResult —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏
    """
    result = ImportResult(archive_name=zip_path.name)
    start_time = time.time()
    
    logger.info("="*80)
    logger.info(f"üì¶ –û–±—Ä–∞–±–æ—Ç–∫–∞: {zip_path.name}")
    logger.info("="*80)
    
    # –®–∞–≥ 1: –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å–µ—Ä–∏–π–Ω–æ–≥–æ –Ω–æ–º–µ—Ä–∞
    logger.info("üîç –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å–µ—Ä–∏–π–Ω–æ–≥–æ –Ω–æ–º–µ—Ä–∞...")
    sn = extract_serial_number(zip_path)
    result.serial_number = sn
    
    if sn:
        logger.info(f"‚úÖ –°–µ—Ä–∏–π–Ω—ã–π –Ω–æ–º–µ—Ä: {sn}")
    else:
        logger.warning(f"‚ö†Ô∏è  –ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å —Å–µ—Ä–∏–π–Ω—ã–π –Ω–æ–º–µ—Ä –∏–∑ {zip_path.name}")
        sn = f"UNKNOWN_{zip_path.stem}"
        result.serial_number = sn
    
    # –®–∞–≥ 2: –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö (–µ—Å–ª–∏ skip_existing)
    if skip_existing and vm_client and sn:
        logger.info("üîç –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –¥–∞–Ω–Ω—ã—Ö –≤ VictoriaMetrics...")
        data_exists, last_date = verify_data_in_vm(vm_client, sn, logger)
        
        if data_exists:
            logger.info(f"‚è≠Ô∏è  –ü—Ä–æ–ø—É—Å–∫: –¥–∞–Ω–Ω—ã–µ —É–∂–µ –µ—Å—Ç—å –≤ VM (–ø–æ—Å–ª–µ–¥–Ω—è—è —Ç–æ—á–∫–∞: {last_date})")
            result.status = "skipped"
            result.data_in_vm = True
            result.last_datapoint = last_date
            result.import_time = time.time() - start_time
            return result
    
    # –®–∞–≥ 3: Dry-run —Ä–µ–∂–∏–º
    if dry_run:
        logger.info("üß™ DRY-RUN —Ä–µ–∂–∏–º: –∏–º–ø–æ—Ä—Ç –Ω–µ –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è")
        result.status = "skipped"
        result.import_time = time.time() - start_time
        return result
    
    # –®–∞–≥ 4: –ó–∞–ø—É—Å–∫ streaming pipeline
    logger.info("üöÄ –ó–∞–ø—É—Å–∫ streaming pipeline...")
    success, output, metrics_sent = run_streaming_pipeline(zip_path, vm_url, logger)
    result.metrics_sent = metrics_sent
    
    if not success:
        logger.error(f"‚ùå –ò–º–ø–æ—Ä—Ç –∑–∞–≤–µ—Ä—à–∏–ª—Å—è —Å –æ—à–∏–±–∫–æ–π")
        result.status = "failed"
        result.error_message = "Pipeline execution failed"
        result.import_time = time.time() - start_time
        return result
    
    logger.info(f"‚úÖ Pipeline –∑–∞–≤–µ—Ä—à–∏–ª—Å—è —É—Å–ø–µ—à–Ω–æ. –ú–µ—Ç—Ä–∏–∫ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–æ: {metrics_sent:,}")
    
    # –®–∞–≥ 5: –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–∞–Ω–Ω—ã—Ö –≤ VM (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω –∫–ª–∏–µ–Ω—Ç)
    if vm_client and sn:
        logger.info("üîç –ü—Ä–æ–≤–µ—Ä–∫–∞ –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –≤ VictoriaMetrics...")
        # –î–∞–µ–º VM –≤—Ä–µ–º—è –Ω–∞ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—é (–Ω–µ–±–æ–ª—å—à–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞)
        time.sleep(2)
        
        data_exists, last_date = verify_data_in_vm(vm_client, sn, logger)
        result.data_in_vm = data_exists
        result.last_datapoint = last_date
        
        if data_exists:
            result.status = "success"
        else:
            result.status = "success"  # Pipeline —É—Å–ø–µ—à–µ–Ω, –Ω–æ –¥–∞–Ω–Ω—ã—Ö –Ω–µ –≤–∏–¥–Ω–æ (–º–æ–∂–µ—Ç –±—ã—Ç—å –∑–∞–¥–µ—Ä–∂–∫–∞ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏)
            logger.warning("‚ö†Ô∏è  –î–∞–Ω–Ω—ã–µ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –≤ VM, –Ω–æ pipeline –≤—ã–ø–æ–ª–Ω–µ–Ω —É—Å–ø–µ—à–Ω–æ")
    else:
        # VM client –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω - —Å—á–∏—Ç–∞–µ–º —É—Å–ø–µ—à–Ω—ã–º –µ—Å–ª–∏ pipeline –∑–∞–≤–µ—Ä—à–∏–ª—Å—è –±–µ–∑ –æ—à–∏–±–æ–∫
        result.status = "success"
        logger.info("‚úÖ –ò–º–ø–æ—Ä—Ç –∑–∞–≤–µ—Ä—à–µ–Ω (–ø—Ä–æ–≤–µ—Ä–∫–∞ VM –ø—Ä–æ–ø—É—â–µ–Ω–∞)")
    
    result.import_time = time.time() - start_time
    logger.info(f"‚è±Ô∏è  –í—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: {result.import_time:.1f}s")
    
    return result


def generate_report(stats: BatchStats, log_filename: str, logger: logging.Logger):
    """
    –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç –ø–æ batch –∏–º–ø–æ—Ä—Ç—É.
    
    Args:
        stats: BatchStats —Å–æ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–æ–π
        log_filename: –ò–º—è –ª–æ–≥-—Ñ–∞–π–ª–∞
        logger: Logger –¥–ª—è –≤—ã–≤–æ–¥–∞
    """
    logger.info("\n" + "="*80)
    logger.info("üìä BATCH IMPORT SUMMARY")
    logger.info("="*80)
    
    # –û—Å–Ω–æ–≤–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    logger.info(f"Total archives found:    {stats.total_archives}")
    logger.info(f"Total archives processed: {stats.processed}/{stats.total_archives}")
    logger.info(f"  - Successful imports:   {stats.successful}")
    logger.info(f"  - Failed imports:       {stats.failed}")
    logger.info(f"  - Skipped:              {stats.skipped}")
    logger.info("")
    
    # –í—Ä–µ–º–µ–Ω–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    hours = int(stats.total_time // 3600)
    minutes = int((stats.total_time % 3600) // 60)
    seconds = int(stats.total_time % 60)
    
    logger.info(f"Total time: {hours}h {minutes}m {seconds}s")
    
    if stats.processed > 0:
        avg_time = stats.total_time / stats.processed
        logger.info(f"Average time per archive: {avg_time:.1f}s")
    logger.info("")
    
    # –ú–∞—Å—Å–∏–≤—ã —Å –¥–∞–Ω–Ω—ã–º–∏ –≤ VM
    arrays_with_data = [r for r in stats.results if r.data_in_vm]
    arrays_without_data = [r for r in stats.results if r.status == "success" and not r.data_in_vm]
    
    if arrays_with_data:
        logger.info(f"Arrays with data in VictoriaMetrics: {len(arrays_with_data)}")
        for result in arrays_with_data[:10]:  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 10
            logger.info(f"  - {result.serial_number} (last data: {result.last_datapoint})")
        if len(arrays_with_data) > 10:
            logger.info(f"  ... –∏ –µ—â–µ {len(arrays_with_data) - 10} –º–∞—Å—Å–∏–≤–æ–≤")
        logger.info("")
    
    # –ú–∞—Å—Å–∏–≤—ã –±–µ–∑ –¥–∞–Ω–Ω—ã—Ö –≤ VM
    if arrays_without_data:
        logger.info(f"Arrays without data in VictoriaMetrics: {len(arrays_without_data)}")
        for result in arrays_without_data:
            logger.info(f"  - {result.serial_number} ({result.archive_name})")
        logger.info("")
    
    # –û—à–∏–±–∫–∏
    failed_results = [r for r in stats.results if r.status == "failed"]
    if failed_results:
        logger.info(f"Failed imports: {len(failed_results)}")
        for result in failed_results:
            logger.info(f"  - {result.archive_name}")
            if result.error_message:
                logger.info(f"    Error: {result.error_message}")
        logger.info("")
    
    # –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –º–µ—Ç—Ä–∏–∫
    total_metrics = sum(r.metrics_sent for r in stats.results)
    if total_metrics > 0:
        logger.info(f"Total metrics sent: {total_metrics:,}")
        logger.info("")
    
    logger.info(f"Details in log: {log_filename}")
    logger.info("="*80)


def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è."""
    global logger
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –æ–±—Ä–∞–±–æ—Ç—á–∏–∫–∞ —Å–∏–≥–Ω–∞–ª–æ–≤
    signal.signal(signal.SIGINT, signal_handler)
    signal.signal(signal.SIGTERM, signal_handler)
    
    # –ü–∞—Ä—Å–∏–Ω–≥ –∞—Ä–≥—É–º–µ–Ω—Ç–æ–≤
    parser = argparse.ArgumentParser(
        description="Batch Import –¥–ª—è –º–∞—Å—Å–æ–≤–æ–≥–æ –∏–º–ø–æ—Ä—Ç–∞ Huawei Performance –ª–æ–≥–æ–≤ –≤ VictoriaMetrics",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
–ü—Ä–∏–º–µ—Ä—ã:

  # –ë–∞–∑–æ–≤—ã–π –∑–∞–ø—É—Å–∫ (–∏–º–ø–æ—Ä—Ç –≤—Å–µ—Ö –∞—Ä—Ö–∏–≤–æ–≤)
  %(prog)s /data/vtb_hc/perf/
  
  # –° –ø—Ä–æ–ø—É—Å–∫–æ–º —É–∂–µ –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö
  %(prog)s /data/vtb_hc/perf/ --skip-existing
  
  # Dry-run (–±–µ–∑ —Ä–µ–∞–ª—å–Ω–æ–≥–æ –∏–º–ø–æ—Ä—Ç–∞)
  %(prog)s /data/vtb_hc/perf/ --dry-run
  
  # –° –∫–∞—Å—Ç–æ–º–Ω—ã–º VM URL
  %(prog)s /data/vtb_hc/perf/ --vm-url http://10.5.10.163:8428
        """
    )
    
    parser.add_argument(
        'log_dir',
        type=str,
        default='/data/vtb_hc/perf/',
        nargs='?',
        help='–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å ZIP –∞—Ä—Ö–∏–≤–∞–º–∏ (default: /data/vtb_hc/perf/)'
    )
    parser.add_argument(
        '--vm-url',
        type=str,
        default='http://localhost:8428',
        help='URL VictoriaMetrics (default: http://localhost:8428)'
    )
    parser.add_argument(
        '--skip-existing',
        action='store_true',
        help='–ü—Ä–æ–ø—É—Å–∫–∞—Ç—å –∞—Ä—Ö–∏–≤—ã, –¥–∞–Ω–Ω—ã–µ –∫–æ—Ç–æ—Ä—ã—Ö —É–∂–µ –µ—Å—Ç—å –≤ VM'
    )
    parser.add_argument(
        '--dry-run',
        action='store_true',
        help='–†–µ–∂–∏–º –±–µ–∑ —Ä–µ–∞–ª—å–Ω–æ–≥–æ –∏–º–ø–æ—Ä—Ç–∞ (—Ç–æ–ª—å–∫–æ –∞–Ω–∞–ª–∏–∑)'
    )
    parser.add_argument(
        '--workers',
        type=int,
        default=None,
        help='–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö workers –¥–ª—è streaming pipeline (–Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤ —Ç–µ–∫—É—â–µ–π –≤–µ—Ä—Å–∏–∏)'
    )
    
    args = parser.parse_args()
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
    logger, log_filename = setup_logging()
    
    logger.info("="*80)
    logger.info("üöÄ BATCH IMPORT STARTED")
    logger.info("="*80)
    logger.info(f"Log directory: {args.log_dir}")
    logger.info(f"VM URL: {args.vm_url}")
    logger.info(f"Skip existing: {args.skip_existing}")
    logger.info(f"Dry-run: {args.dry_run}")
    logger.info("="*80)
    logger.info("")
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
    log_dir_path = Path(args.log_dir)
    if not log_dir_path.exists():
        logger.error(f"‚ùå –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {log_dir_path}")
        sys.exit(1)
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è VM –∫–ª–∏–µ–Ω—Ç–∞
    vm_client = None
    if VM_CLIENT_AVAILABLE:
        try:
            vm_client = VictoriaMetricsClient(vm_url=args.vm_url)
            if vm_client.check_availability():
                logger.info("‚úÖ VictoriaMetrics –¥–æ—Å—Ç—É–ø–Ω–∞")
            else:
                logger.warning("‚ö†Ô∏è  VictoriaMetrics –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞, –ø—Ä–æ–≤–µ—Ä–∫–∞ –¥–∞–Ω–Ω—ã—Ö –±—É–¥–µ—Ç –ø—Ä–æ–ø—É—â–µ–Ω–∞")
                vm_client = None
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è  –ù–µ —É–¥–∞–ª–æ—Å—å –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å VM –∫–ª–∏–µ–Ω—Ç: {e}")
            vm_client = None
    else:
        logger.warning("‚ö†Ô∏è  VictoriaMetricsClient –Ω–µ –¥–æ—Å—Ç—É–ø–µ–Ω, –ø—Ä–æ–≤–µ—Ä–∫–∞ –¥–∞–Ω–Ω—ã—Ö –±—É–¥–µ—Ç –ø—Ä–æ–ø—É—â–µ–Ω–∞")
    
    # –ü–æ–∏—Å–∫ ZIP —Ñ–∞–π–ª–æ–≤
    logger.info("üîç –ü–æ–∏—Å–∫ ZIP –∞—Ä—Ö–∏–≤–æ–≤...")
    zip_files = sorted(log_dir_path.glob("*.zip"))
    
    if not zip_files:
        logger.error(f"‚ùå ZIP —Ñ–∞–π–ª—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –≤ {log_dir_path}")
        sys.exit(1)
    
    logger.info(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(zip_files)} ZIP –∞—Ä—Ö–∏–≤–æ–≤")
    logger.info("")
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
    stats = BatchStats(total_archives=len(zip_files))
    start_time = time.time()
    
    # –û–±—Ä–∞–±–æ—Ç–∫–∞ –∞—Ä—Ö–∏–≤–æ–≤
    for idx, zip_file in enumerate(zip_files, 1):
        if INTERRUPTED:
            logger.warning("‚ö†Ô∏è  –ü—Ä–µ—Ä–≤–∞–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
            break
        
        logger.info(f"[{idx}/{len(zip_files)}] –û–±—Ä–∞–±–æ—Ç–∫–∞ {zip_file.name}...")
        
        try:
            result = process_archive(
                zip_file,
                args.vm_url,
                vm_client,
                args.skip_existing,
                args.dry_run,
                logger
            )
            
            stats.results.append(result)
            stats.processed += 1
            
            if result.status == "success":
                stats.successful += 1
            elif result.status == "failed":
                stats.failed += 1
            elif result.status == "skipped":
                stats.skipped += 1
            
        except Exception as e:
            logger.error(f"‚ùå –ù–µ–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ {zip_file.name}: {e}")
            result = ImportResult(
                archive_name=zip_file.name,
                status="failed",
                error_message=str(e)
            )
            stats.results.append(result)
            stats.processed += 1
            stats.failed += 1
        
        logger.info("")
    
    # –§–∏–Ω–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    stats.total_time = time.time() - start_time
    
    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–∞
    generate_report(stats, log_filename, logger)
    
    # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –∫–æ–¥ –≤—ã—Ö–æ–¥–∞
    if stats.failed > 0:
        sys.exit(1)
    else:
        sys.exit(0)


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n‚ö†Ô∏è  –ü—Ä–µ—Ä–≤–∞–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
        sys.exit(130)
    except Exception as e:
        print(f"\n‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

