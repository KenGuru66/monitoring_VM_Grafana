#!/usr/bin/env python3
"""
Batch Import Script –¥–ª—è –º–∞—Å—Å–æ–≤–æ–≥–æ –∏–º–ø–æ—Ä—Ç–∞ Huawei Performance –ª–æ–≥–æ–≤ –≤ VictoriaMetrics

–°–∫—Ä–∏–ø—Ç –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç ZIP –∞—Ä—Ö–∏–≤—ã —Å performance –¥–∞–Ω–Ω—ã–º–∏,
–∑–∞–ø—É—Å–∫–∞–µ—Ç streaming pipeline –¥–ª—è –∏–º–ø–æ—Ä—Ç–∞ –≤ VictoriaMetrics –∏ –ø—Ä–æ–≤–µ—Ä—è–µ—Ç
—É—Å–ø–µ—à–Ω–æ—Å—Ç—å –∏–º–ø–æ—Ä—Ç–∞.

–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:
    python3 batch_import.py /path/to/logs/
    python3 batch_import.py /path/to/logs/ --skip-existing
    python3 batch_import.py /path/to/logs/ --dry-run
"""

import sys
import os
import re
import zipfile
import subprocess
import argparse
import logging
import time
import signal
from pathlib import Path
from datetime import datetime, timedelta
from typing import Optional, Tuple, List, Dict, Any
from dataclasses import dataclass, field
import shutil

# –ü–æ–¥–¥–µ—Ä–∂–∫–∞ .7z –∞—Ä—Ö–∏–≤–æ–≤
try:
    import py7zr
    PY7ZR_AVAILABLE = True
except ImportError:
    PY7ZR_AVAILABLE = False

# –ò–º–ø–æ—Ä—Ç VictoriaMetricsClient –∏–∑ tools/
try:
    from tools.victoriametrics_client import VictoriaMetricsClient
    VM_CLIENT_AVAILABLE = True
except ImportError:
    try:
        # Fallback –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –∏–∑ –∫–æ—Ä–Ω—è –ø—Ä–æ–µ–∫—Ç–∞
        from victoriametrics_client import VictoriaMetricsClient
        VM_CLIENT_AVAILABLE = True
    except ImportError:
        VM_CLIENT_AVAILABLE = False
        print("Warning: VictoriaMetricsClient not available, verification will be skipped")


@dataclass
class ImportResult:
    """–†–µ–∑—É–ª—å—Ç–∞—Ç –∏–º–ø–æ—Ä—Ç–∞ –æ–¥–Ω–æ–≥–æ –∞—Ä—Ö–∏–≤–∞."""
    archive_name: str
    serial_number: Optional[str] = None
    status: str = "pending"  # pending, success, failed, skipped
    import_time: float = 0.0
    error_message: Optional[str] = None
    data_in_vm: bool = False
    last_datapoint: Optional[str] = None
    metrics_sent: int = 0
    
    
@dataclass
class BatchStats:
    """–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ batch –∏–º–ø–æ—Ä—Ç–∞."""
    total_archives: int = 0
    processed: int = 0
    successful: int = 0
    failed: int = 0
    skipped: int = 0
    total_time: float = 0.0
    results: List[ImportResult] = field(default_factory=list)
    

# –ì–ª–æ–±–∞–ª—å–Ω–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è –¥–ª—è graceful shutdown
INTERRUPTED = False


def signal_handler(signum, frame):
    """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ —Å–∏–≥–Ω–∞–ª–∞ –ø—Ä–µ—Ä—ã–≤–∞–Ω–∏—è (Ctrl+C)."""
    global INTERRUPTED
    INTERRUPTED = True
    logger.warning("\n‚ö†Ô∏è  –ü—Ä–µ—Ä—ã–≤–∞–Ω–∏–µ –ø–æ–ª—É—á–µ–Ω–æ! –ó–∞–≤–µ—Ä—à–∞—é —Ç–µ–∫—É—â—É—é –æ–ø–µ—Ä–∞—Ü–∏—é...")


def setup_logging(log_dir: Path = Path(".")) -> Tuple[logging.Logger, str]:
    """
    –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è.
    
    Args:
        log_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è –ª–æ–≥-—Ñ–∞–π–ª–∞
        
    Returns:
        Tuple[Logger, –∏–º—è –ª–æ–≥-—Ñ–∞–π–ª–∞]
    """
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_filename = f"batch_import_{timestamp}.log"
    log_path = log_dir / log_filename
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
    log_format = '%(asctime)s - %(levelname)s - %(message)s'
    
    # –°–æ–∑–¥–∞–µ–º logger
    logger = logging.getLogger('batch_import')
    logger.setLevel(logging.DEBUG)
    
    # File handler
    file_handler = logging.FileHandler(log_path, mode='w', encoding='utf-8')
    file_handler.setLevel(logging.DEBUG)
    file_handler.setFormatter(logging.Formatter(log_format))
    
    # Console handler
    console_handler = logging.StreamHandler(sys.stdout)
    console_handler.setLevel(logging.INFO)
    console_handler.setFormatter(logging.Formatter(log_format))
    
    # –î–æ–±–∞–≤–ª—è–µ–º handlers
    logger.addHandler(file_handler)
    logger.addHandler(console_handler)
    
    return logger, log_filename


def extract_serial_number(archive_path: Path) -> Optional[str]:
    """
    –ò–∑–≤–ª–µ–∫–∞–µ—Ç —Å–µ—Ä–∏–π–Ω—ã–π –Ω–æ–º–µ—Ä –º–∞—Å—Å–∏–≤–∞ –∏–∑ –∏–º–µ–Ω–∏ .tgz —Ñ–∞–π–ª–æ–≤ –≤–Ω—É—Ç—Ä–∏ –∞—Ä—Ö–∏–≤–∞.
    
    –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç .zip –∏ .7z –∞—Ä—Ö–∏–≤—ã.
    –ò—â–µ—Ç –ø–∞—Ç—Ç–µ—Ä–Ω: PerfData_*_SN_<SERIAL>_SP* –∏–ª–∏ –∏–∑–≤–ª–µ–∫–∞–µ—Ç –∏–∑ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞.
    
    Args:
        archive_path: –ü—É—Ç—å –∫ –∞—Ä—Ö–∏–≤—É (.zip –∏–ª–∏ .7z)
        
    Returns:
        –°–µ—Ä–∏–π–Ω—ã–π –Ω–æ–º–µ—Ä –∏–ª–∏ None –µ—Å–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω
    """
    try:
        file_list = []
        suffix = archive_path.suffix.lower()
        
        # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ —Ñ–∞–π–ª–æ–≤ –≤ –∞—Ä—Ö–∏–≤–µ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ñ–æ—Ä–º–∞—Ç–∞
        if suffix == '.zip':
            with zipfile.ZipFile(archive_path, 'r') as zip_ref:
                file_list = zip_ref.namelist()
        elif suffix == '.7z':
            if not PY7ZR_AVAILABLE:
                logger.warning(f"py7zr –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω, –Ω–µ –º–æ–≥—É –ø—Ä–æ—á–∏—Ç–∞—Ç—å .7z –∞—Ä—Ö–∏–≤")
            else:
                with py7zr.SevenZipFile(archive_path, mode='r') as archive:
                    file_list = archive.getnames()
        
        # –ò—â–µ–º .tgz —Ñ–∞–π–ª—ã —Å –ø–∞—Ç—Ç–µ—Ä–Ω–æ–º SN
        pattern = r"PerfData_.*_SN_([0-9A-Z]+)_SP"
        
        for filename in file_list:
            if filename.endswith('.tgz'):
                match = re.search(pattern, filename)
                if match:
                    return match.group(1)
        
        # Fallback 1: –ò—â–µ–º —Å–µ—Ä–∏–π–Ω—ã–π –Ω–æ–º–µ—Ä –≤ –∏–º–µ–Ω–∏ –∞—Ä—Ö–∏–≤–∞
        # –§–æ—Ä–º–∞—Ç –∏–º–µ–Ω–∏: Data_<Model>_<Timestamp>_<SN>.7z
        # –ù–∞–ø—Ä–∏–º–µ—Ä: Data_Dorado6000V3_20251023163227_2102352KRR10KC000013.7z
        # SN –≤—Å–µ–≥–¥–∞ –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å "21" –∏ —Å–æ–¥–µ—Ä–∂–∏—Ç 20+ —Å–∏–º–≤–æ–ª–æ–≤ (—Ü–∏—Ñ—Ä—ã –∏ –±—É–∫–≤—ã)
        match = re.search(r"_(21[0-9A-Z]{18,})\.(zip|7z)$", archive_path.name, re.IGNORECASE)
        if match:
            return match.group(1)
        
        # Fallback 2: –°–µ—Ä–∏–π–Ω—ã–π –Ω–æ–º–µ—Ä –≤ –∏–º–µ–Ω–∏ —Ä–æ–¥–∏—Ç–µ–ª—å—Å–∫–æ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
        # –§–æ—Ä–º–∞—Ç: IP_SN (–Ω–∞–ø—Ä–∏–º–µ—Ä: 10.105.152.20_2102352KRR10KC000013)
        parent_name = archive_path.parent.name
        match = re.search(r"_(21[0-9A-Z]{18,})$", parent_name)
        if match:
            return match.group(1)
        
        # Fallback 3: –ø–æ–ø—Ä–æ–±—É–µ–º –∏–∑–≤–ª–µ—á—å IP –∏–∑ –∏–º–µ–Ω–∏ (—Å—Ç–∞—Ä—ã–π —Ñ–æ—Ä–º–∞—Ç)
        match = re.search(r"\(([0-9.]+)\)", archive_path.name)
        if match:
            return match.group(1).replace(".", "_")
        
        return None
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ SN –∏–∑ {archive_path.name}: {e}")
        return None


def extract_perf_zip_from_7z(archive_path: Path, temp_dir: Path, logger: logging.Logger) -> Optional[Path]:
    """
    –ò–∑–≤–ª–µ–∫–∞–µ—Ç Perf ZIP —Ñ–∞–π–ª –∏–∑ .7z –∞—Ä—Ö–∏–≤–∞.
    
    –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –≤–Ω—É—Ç—Ä–∏ .7z:
    DataCollect/History_Performance_Data/<IP>/(<IP>)..._Perf_*.zip
    
    Args:
        archive_path: –ü—É—Ç—å –∫ .7z –∞—Ä—Ö–∏–≤—É
        temp_dir: –í—Ä–µ–º–µ–Ω–Ω–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è
        logger: Logger
        
    Returns:
        –ü—É—Ç—å –∫ –∏–∑–≤–ª–µ—á—ë–Ω–Ω–æ–º—É .zip —Ñ–∞–π–ª—É –∏–ª–∏ None
    """
    if not PY7ZR_AVAILABLE:
        logger.error("‚ùå py7zr –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω! –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install py7zr")
        return None
    
    try:
        with py7zr.SevenZipFile(archive_path, mode='r') as archive:
            all_names = archive.getnames()
            
            # –ò—â–µ–º —Ñ–∞–π–ª—ã —Å –ø–∞—Ç—Ç–µ—Ä–Ω–æ–º *_Perf_*.zip –≤ History_Performance_Data
            perf_zip_files = [
                name for name in all_names 
                if '_Perf_' in name and name.endswith('.zip') and 'History_Performance_Data' in name
            ]
            
            if not perf_zip_files:
                logger.warning(f"‚ö†Ô∏è  Perf ZIP —Ñ–∞–π–ª—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –≤–Ω—É—Ç—Ä–∏ {archive_path.name}")
                return None
            
            if len(perf_zip_files) > 1:
                logger.info(f"üì¶ –ù–∞–π–¥–µ–Ω–æ {len(perf_zip_files)} Perf ZIP —Ñ–∞–π–ª–æ–≤, –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø–µ—Ä–≤—ã–π")
            
            perf_zip_name = perf_zip_files[0]
            logger.info(f"üì¶ –ò–∑–≤–ª–µ–∫–∞—é: {perf_zip_name}")
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–æ–ª—å–∫–æ –Ω—É–∂–Ω—ã–π —Ñ–∞–π–ª
            archive.extract(temp_dir, targets=[perf_zip_name])
            
            extracted_path = temp_dir / perf_zip_name
            if extracted_path.exists():
                return extracted_path
            else:
                logger.error(f"‚ùå –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω –ø–æ—Å–ª–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è: {extracted_path}")
                return None
                
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ –∏–∑ .7z: {e}")
        return None


def run_streaming_pipeline(zip_path: Path, vm_url: str, logger: logging.Logger) -> Tuple[bool, str, int]:
    """
    –ó–∞–ø—É—Å–∫–∞–µ—Ç huawei_streaming_pipeline.py —á–µ—Ä–µ–∑ subprocess.
    
    Args:
        zip_path: –ü—É—Ç—å –∫ ZIP –∞—Ä—Ö–∏–≤—É
        vm_url: URL VictoriaMetrics
        logger: Logger –¥–ª—è –≤—ã–≤–æ–¥–∞
        
    Returns:
        Tuple[—É—Å–ø–µ—Ö, –ª–æ–≥–∏, –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –º–µ—Ç—Ä–∏–∫]
    """
    # –ü—É—Ç—å –∫ streaming pipeline (–≤ parsers/ –∏–ª–∏ –∫–æ—Ä–Ω–µ –¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏)
    pipeline_script = Path(__file__).parent.parent / "parsers" / "streaming_pipeline.py"
    if not pipeline_script.exists():
        # Fallback –Ω–∞ —Å—Ç–∞—Ä—ã–π –ø—É—Ç—å
        pipeline_script = Path(__file__).parent.parent / "huawei_streaming_pipeline.py"
    
    if not pipeline_script.exists():
        logger.error(f"–ù–µ –Ω–∞–π–¥–µ–Ω —Å–∫—Ä–∏–ø—Ç: {pipeline_script}")
        return False, "Pipeline script not found", 0
    
    # –§–æ—Ä–º–∏—Ä—É–µ–º –∫–æ–º–∞–Ω–¥—É
    cmd = [
        sys.executable,
        str(pipeline_script),
        "-i", str(zip_path),
        "--vm-url", f"{vm_url}/api/v1/import/prometheus",
        "--monitor"
    ]
    
    logger.info(f"–ó–∞–ø—É—Å–∫: {' '.join(cmd)}")
    
    try:
        # –ó–∞–ø—É—Å–∫–∞–µ–º –ø—Ä–æ—Ü–µ—Å—Å —Å –ø–µ—Ä–µ—Ö–≤–∞—Ç–æ–º –≤—ã–≤–æ–¥–∞
        process = subprocess.Popen(
            cmd,
            stdout=subprocess.PIPE,
            stderr=subprocess.STDOUT,
            universal_newlines=True,
            bufsize=1
        )
        
        output_lines = []
        metrics_sent = 0
        
        # –ß–∏—Ç–∞–µ–º –≤—ã–≤–æ–¥ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏
        for line in process.stdout:
            output_lines.append(line)
            
            # –õ–æ–≥–∏—Ä—É–µ–º –≤–∞–∂–Ω—ã–µ —Å—Ç—Ä–æ–∫–∏
            if "Processing" in line or "‚úÖ" in line or "ERROR" in line or "WARNING" in line:
                logger.debug(line.strip())
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫
            if "Metrics sent:" in line:
                match = re.search(r"Metrics sent:\s+([\d,]+)", line)
                if match:
                    metrics_sent = int(match.group(1).replace(",", ""))
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø—Ä–µ—Ä—ã–≤–∞–Ω–∏–µ
            if INTERRUPTED:
                process.terminate()
                process.wait(timeout=5)
                return False, "Interrupted by user", 0
        
        # –ñ–¥–µ–º –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è
        return_code = process.wait()
        
        full_output = "".join(output_lines)
        
        if return_code == 0:
            logger.info(f"‚úÖ Pipeline –∑–∞–≤–µ—Ä—à–∏–ª—Å—è —É—Å–ø–µ—à–Ω–æ")
            return True, full_output, metrics_sent
        else:
            logger.error(f"‚ùå Pipeline –∑–∞–≤–µ—Ä—à–∏–ª—Å—è —Å –æ—à–∏–±–∫–æ–π (–∫–æ–¥: {return_code})")
            return False, full_output, metrics_sent
            
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø—É—Å–∫–µ pipeline: {e}")
        return False, str(e), 0


def verify_data_in_vm(client: VictoriaMetricsClient, sn: str, logger: logging.Logger) -> Tuple[bool, Optional[str]]:
    """
    –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –Ω–∞–ª–∏—á–∏–µ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–µ—Ä–∏–π–Ω–æ–≥–æ –Ω–æ–º–µ—Ä–∞ –≤ VictoriaMetrics.
    
    Args:
        client: VictoriaMetricsClient
        sn: –°–µ—Ä–∏–π–Ω—ã–π –Ω–æ–º–µ—Ä –º–∞—Å—Å–∏–≤–∞
        logger: Logger –¥–ª—è –≤—ã–≤–æ–¥–∞
        
    Returns:
        Tuple[–¥–∞–Ω–Ω—ã–µ –µ—Å—Ç—å, –¥–∞—Ç–∞ –ø–æ—Å–ª–µ–¥–Ω–µ–π —Ç–æ—á–∫–∏]
    """
    try:
        last_timestamp = client.get_last_datapoint_time(sn)
        
        if last_timestamp:
            last_date = datetime.fromtimestamp(last_timestamp).strftime('%Y-%m-%d %H:%M:%S')
            logger.info(f"‚úÖ –î–∞–Ω–Ω—ã–µ –≤ VM –Ω–∞–π–¥–µ–Ω—ã. –ü–æ—Å–ª–µ–¥–Ω—è—è —Ç–æ—á–∫–∞: {last_date}")
            return True, last_date
        else:
            logger.warning(f"‚ö†Ô∏è  –î–∞–Ω–Ω—ã–µ –¥–ª—è SN {sn} –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –≤ VictoriaMetrics")
            return False, None
            
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ –¥–∞–Ω–Ω—ã—Ö –≤ VM: {e}")
        return False, None


def process_archive(
    archive_path: Path,
    vm_url: str,
    vm_client: Optional[VictoriaMetricsClient],
    skip_existing: bool,
    dry_run: bool,
    logger: logging.Logger
) -> ImportResult:
    """
    –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ–¥–Ω–æ–≥–æ –∞—Ä—Ö–∏–≤–∞.
    
    –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç:
    - .zip —Ñ–∞–π–ª—ã —Å Perf –¥–∞–Ω–Ω—ã–º–∏ (–ø—Ä—è–º–æ–π –∑–∞–ø—É—Å–∫ pipeline)
    - .7z —Ñ–∞–π–ª—ã —Å –≤–ª–æ–∂–µ–Ω–Ω–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä–æ–π (–∏–∑–≤–ª–µ—á–µ–Ω–∏–µ Perf .zip –∏ –∑–∞–ø—É—Å–∫)
    
    –°—Ç—Ä—É–∫—Ç—É—Ä–∞ .7z:
    DataCollect/History_Performance_Data/<IP>/(<IP>)..._Perf_*.zip ‚Üí .tgz —Ñ–∞–π–ª—ã
    
    Args:
        archive_path: –ü—É—Ç—å –∫ –∞—Ä—Ö–∏–≤—É (.zip –∏–ª–∏ .7z)
        vm_url: URL VictoriaMetrics
        vm_client: VictoriaMetricsClient –∏–ª–∏ None
        skip_existing: –ü—Ä–æ–ø—É—Å–∫–∞—Ç—å –µ—Å–ª–∏ –¥–∞–Ω–Ω—ã–µ —É–∂–µ –µ—Å—Ç—å –≤ VM
        dry_run: –†–µ–∂–∏–º –±–µ–∑ —Ä–µ–∞–ª—å–Ω–æ–≥–æ –∏–º–ø–æ—Ä—Ç–∞
        logger: Logger
        
    Returns:
        ImportResult —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏
    """
    result = ImportResult(archive_name=archive_path.name)
    start_time = time.time()
    temp_dir = None
    
    logger.info("="*80)
    logger.info(f"üì¶ –û–±—Ä–∞–±–æ—Ç–∫–∞: {archive_path.name}")
    logger.info("="*80)
    
    try:
        # –®–∞–≥ 1: –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å–µ—Ä–∏–π–Ω–æ–≥–æ –Ω–æ–º–µ—Ä–∞
        logger.info("üîç –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å–µ—Ä–∏–π–Ω–æ–≥–æ –Ω–æ–º–µ—Ä–∞...")
        sn = extract_serial_number(archive_path)
        result.serial_number = sn
        
        if sn:
            logger.info(f"‚úÖ –°–µ—Ä–∏–π–Ω—ã–π –Ω–æ–º–µ—Ä: {sn}")
        else:
            logger.warning(f"‚ö†Ô∏è  –ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å —Å–µ—Ä–∏–π–Ω—ã–π –Ω–æ–º–µ—Ä –∏–∑ {archive_path.name}")
            sn = f"UNKNOWN_{archive_path.stem}"
            result.serial_number = sn
        
        # –®–∞–≥ 2: –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö (–µ—Å–ª–∏ skip_existing)
        if skip_existing and vm_client and sn and not sn.startswith("UNKNOWN_"):
            logger.info("üîç –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –¥–∞–Ω–Ω—ã—Ö –≤ VictoriaMetrics...")
            data_exists, last_date = verify_data_in_vm(vm_client, sn, logger)
            
            if data_exists:
                logger.info(f"‚è≠Ô∏è  –ü—Ä–æ–ø—É—Å–∫: –¥–∞–Ω–Ω—ã–µ —É–∂–µ –µ—Å—Ç—å –≤ VM (–ø–æ—Å–ª–µ–¥–Ω—è—è —Ç–æ—á–∫–∞: {last_date})")
                result.status = "skipped"
                result.data_in_vm = True
                result.last_datapoint = last_date
                result.import_time = time.time() - start_time
                return result
        
        # –®–∞–≥ 3: Dry-run —Ä–µ–∂–∏–º
        if dry_run:
            logger.info("üß™ DRY-RUN —Ä–µ–∂–∏–º: –∏–º–ø–æ—Ä—Ç –Ω–µ –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è")
            result.status = "skipped"
            result.import_time = time.time() - start_time
            return result
        
        # –®–∞–≥ 4: –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –∞—Ä—Ö–∏–≤–∞ –∏ –ø–æ–ª—É—á–∞–µ–º –ø—É—Ç—å –∫ Perf ZIP
        suffix = archive_path.suffix.lower()
        perf_zip_path = None
        
        if suffix == '.zip':
            # –ü—Ä—è–º–æ–π .zip —Ñ–∞–π–ª —Å Perf –¥–∞–Ω–Ω—ã–º–∏
            perf_zip_path = archive_path
            logger.info(f"üì¶ –¢–∏–ø: ZIP (–ø—Ä—è–º–æ–π Perf —Ñ–∞–π–ª)")
            
        elif suffix == '.7z':
            # .7z –∞—Ä—Ö–∏–≤ —Å –≤–ª–æ–∂–µ–Ω–Ω–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä–æ–π - –∏–∑–≤–ª–µ–∫–∞–µ–º Perf ZIP
            logger.info(f"üì¶ –¢–∏–ø: 7z (–≤–ª–æ–∂–µ–Ω–Ω–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞)")
            
            # –°–æ–∑–¥–∞—ë–º –≤—Ä–µ–º–µ–Ω–Ω—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è
            temp_dir = Path(f"temp_batch_extract_{archive_path.stem}")
            if temp_dir.exists():
                shutil.rmtree(temp_dir)
            temp_dir.mkdir()
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º Perf ZIP –∏–∑ .7z
            perf_zip_path = extract_perf_zip_from_7z(archive_path, temp_dir, logger)
            
            if not perf_zip_path:
                logger.error(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å Perf ZIP –∏–∑ {archive_path.name}")
                result.status = "failed"
                result.error_message = "Failed to extract Perf ZIP from 7z"
                result.import_time = time.time() - start_time
                return result
        else:
            logger.error(f"‚ùå –ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ñ–æ—Ä–º–∞—Ç: {suffix}")
            result.status = "failed"
            result.error_message = f"Unsupported format: {suffix}"
            result.import_time = time.time() - start_time
            return result
        
        # –®–∞–≥ 5: –ó–∞–ø—É—Å–∫ streaming pipeline
        logger.info("üöÄ –ó–∞–ø—É—Å–∫ streaming pipeline...")
        success, output, metrics_sent = run_streaming_pipeline(perf_zip_path, vm_url, logger)
        result.metrics_sent = metrics_sent
        
        if not success:
            logger.error(f"‚ùå –ò–º–ø–æ—Ä—Ç –∑–∞–≤–µ—Ä—à–∏–ª—Å—è —Å –æ—à–∏–±–∫–æ–π")
            result.status = "failed"
            result.error_message = "Pipeline execution failed"
            result.import_time = time.time() - start_time
            return result
        
        logger.info(f"‚úÖ Pipeline –∑–∞–≤–µ—Ä—à–∏–ª—Å—è —É—Å–ø–µ—à–Ω–æ. –ú–µ—Ç—Ä–∏–∫ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–æ: {metrics_sent:,}")
        
        # –®–∞–≥ 6: –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–∞–Ω–Ω—ã—Ö –≤ VM (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω –∫–ª–∏–µ–Ω—Ç)
        if vm_client and sn and not sn.startswith("UNKNOWN_"):
            logger.info("üîç –ü—Ä–æ–≤–µ—Ä–∫–∞ –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –≤ VictoriaMetrics...")
            # –î–∞–µ–º VM –≤—Ä–µ–º—è –Ω–∞ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—é (–Ω–µ–±–æ–ª—å—à–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞)
            time.sleep(2)
            
            data_exists, last_date = verify_data_in_vm(vm_client, sn, logger)
            result.data_in_vm = data_exists
            result.last_datapoint = last_date
            
            if data_exists:
                result.status = "success"
            else:
                result.status = "success"  # Pipeline —É—Å–ø–µ—à–µ–Ω, –Ω–æ –¥–∞–Ω–Ω—ã—Ö –Ω–µ –≤–∏–¥–Ω–æ (–º–æ–∂–µ—Ç –±—ã—Ç—å –∑–∞–¥–µ—Ä–∂–∫–∞ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏)
                logger.warning("‚ö†Ô∏è  –î–∞–Ω–Ω—ã–µ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –≤ VM, –Ω–æ pipeline –≤—ã–ø–æ–ª–Ω–µ–Ω —É—Å–ø–µ—à–Ω–æ")
        else:
            # VM client –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω - —Å—á–∏—Ç–∞–µ–º —É—Å–ø–µ—à–Ω—ã–º –µ—Å–ª–∏ pipeline –∑–∞–≤–µ—Ä—à–∏–ª—Å—è –±–µ–∑ –æ—à–∏–±–æ–∫
            result.status = "success"
            logger.info("‚úÖ –ò–º–ø–æ—Ä—Ç –∑–∞–≤–µ—Ä—à–µ–Ω (–ø—Ä–æ–≤–µ—Ä–∫–∞ VM –ø—Ä–æ–ø—É—â–µ–Ω–∞)")
        
        result.import_time = time.time() - start_time
        logger.info(f"‚è±Ô∏è  –í—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: {result.import_time:.1f}s")
        
        return result
        
    finally:
        # Cleanup: —É–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã
        if temp_dir and temp_dir.exists():
            try:
                shutil.rmtree(temp_dir)
                logger.debug(f"üßπ –í—Ä–µ–º–µ–Ω–Ω–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —É–¥–∞–ª–µ–Ω–∞: {temp_dir}")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è  –ù–µ —É–¥–∞–ª–æ—Å—å —É–¥–∞–ª–∏—Ç—å –≤—Ä–µ–º–µ–Ω–Ω—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é: {e}")


def generate_report(stats: BatchStats, log_filename: str, logger: logging.Logger):
    """
    –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç –ø–æ batch –∏–º–ø–æ—Ä—Ç—É.
    
    Args:
        stats: BatchStats —Å–æ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–æ–π
        log_filename: –ò–º—è –ª–æ–≥-—Ñ–∞–π–ª–∞
        logger: Logger –¥–ª—è –≤—ã–≤–æ–¥–∞
    """
    logger.info("\n" + "="*80)
    logger.info("üìä BATCH IMPORT SUMMARY")
    logger.info("="*80)
    
    # –û—Å–Ω–æ–≤–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    logger.info(f"Total archives found:    {stats.total_archives}")
    logger.info(f"Total archives processed: {stats.processed}/{stats.total_archives}")
    logger.info(f"  - Successful imports:   {stats.successful}")
    logger.info(f"  - Failed imports:       {stats.failed}")
    logger.info(f"  - Skipped:              {stats.skipped}")
    logger.info("")
    
    # –í—Ä–µ–º–µ–Ω–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    hours = int(stats.total_time // 3600)
    minutes = int((stats.total_time % 3600) // 60)
    seconds = int(stats.total_time % 60)
    
    logger.info(f"Total time: {hours}h {minutes}m {seconds}s")
    
    if stats.processed > 0:
        avg_time = stats.total_time / stats.processed
        logger.info(f"Average time per archive: {avg_time:.1f}s")
    logger.info("")
    
    # –ú–∞—Å—Å–∏–≤—ã —Å –¥–∞–Ω–Ω—ã–º–∏ –≤ VM
    arrays_with_data = [r for r in stats.results if r.data_in_vm]
    arrays_without_data = [r for r in stats.results if r.status == "success" and not r.data_in_vm]
    
    if arrays_with_data:
        logger.info(f"Arrays with data in VictoriaMetrics: {len(arrays_with_data)}")
        for result in arrays_with_data[:10]:  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 10
            logger.info(f"  - {result.serial_number} (last data: {result.last_datapoint})")
        if len(arrays_with_data) > 10:
            logger.info(f"  ... –∏ –µ—â–µ {len(arrays_with_data) - 10} –º–∞—Å—Å–∏–≤–æ–≤")
        logger.info("")
    
    # –ú–∞—Å—Å–∏–≤—ã –±–µ–∑ –¥–∞–Ω–Ω—ã—Ö –≤ VM
    if arrays_without_data:
        logger.info(f"Arrays without data in VictoriaMetrics: {len(arrays_without_data)}")
        for result in arrays_without_data:
            logger.info(f"  - {result.serial_number} ({result.archive_name})")
        logger.info("")
    
    # –û—à–∏–±–∫–∏
    failed_results = [r for r in stats.results if r.status == "failed"]
    if failed_results:
        logger.info(f"Failed imports: {len(failed_results)}")
        for result in failed_results:
            logger.info(f"  - {result.archive_name}")
            if result.error_message:
                logger.info(f"    Error: {result.error_message}")
        logger.info("")
    
    # –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –º–µ—Ç—Ä–∏–∫
    total_metrics = sum(r.metrics_sent for r in stats.results)
    if total_metrics > 0:
        logger.info(f"Total metrics sent: {total_metrics:,}")
        logger.info("")
    
    logger.info(f"Details in log: {log_filename}")
    logger.info("="*80)


def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è."""
    global logger
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –æ–±—Ä–∞–±–æ—Ç—á–∏–∫–∞ —Å–∏–≥–Ω–∞–ª–æ–≤
    signal.signal(signal.SIGINT, signal_handler)
    signal.signal(signal.SIGTERM, signal_handler)
    
    # –ü–∞—Ä—Å–∏–Ω–≥ –∞—Ä–≥—É–º–µ–Ω—Ç–æ–≤
    parser = argparse.ArgumentParser(
        description="Batch Import –¥–ª—è –º–∞—Å—Å–æ–≤–æ–≥–æ –∏–º–ø–æ—Ä—Ç–∞ Huawei Performance –ª–æ–≥–æ–≤ –≤ VictoriaMetrics",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
–ü—Ä–∏–º–µ—Ä—ã:

  # –ë–∞–∑–æ–≤—ã–π –∑–∞–ø—É—Å–∫ (–∏–º–ø–æ—Ä—Ç –≤—Å–µ—Ö –∞—Ä—Ö–∏–≤–æ–≤)
  %(prog)s /data/vtb_hc/perf/
  
  # –° –ø—Ä–æ–ø—É—Å–∫–æ–º —É–∂–µ –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö
  %(prog)s /data/vtb_hc/perf/ --skip-existing
  
  # Dry-run (–±–µ–∑ —Ä–µ–∞–ª—å–Ω–æ–≥–æ –∏–º–ø–æ—Ä—Ç–∞)
  %(prog)s /data/vtb_hc/perf/ --dry-run
  
  # –° –∫–∞—Å—Ç–æ–º–Ω—ã–º VM URL
  %(prog)s /data/vtb_hc/perf/ --vm-url http://10.5.10.163:8428
        """
    )
    
    parser.add_argument(
        'log_dir',
        type=str,
        default='/data/vtb_hc/perf/',
        nargs='?',
        help='–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å ZIP –∞—Ä—Ö–∏–≤–∞–º–∏ (default: /data/vtb_hc/perf/)'
    )
    parser.add_argument(
        '--vm-url',
        type=str,
        default='http://localhost:8428',
        help='URL VictoriaMetrics (default: http://localhost:8428)'
    )
    parser.add_argument(
        '--skip-existing',
        action='store_true',
        help='–ü—Ä–æ–ø—É—Å–∫–∞—Ç—å –∞—Ä—Ö–∏–≤—ã, –¥–∞–Ω–Ω—ã–µ –∫–æ—Ç–æ—Ä—ã—Ö —É–∂–µ –µ—Å—Ç—å –≤ VM'
    )
    parser.add_argument(
        '--dry-run',
        action='store_true',
        help='–†–µ–∂–∏–º –±–µ–∑ —Ä–µ–∞–ª—å–Ω–æ–≥–æ –∏–º–ø–æ—Ä—Ç–∞ (—Ç–æ–ª—å–∫–æ –∞–Ω–∞–ª–∏–∑)'
    )
    parser.add_argument(
        '--workers',
        type=int,
        default=None,
        help='–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö workers –¥–ª—è streaming pipeline (–Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤ —Ç–µ–∫—É—â–µ–π –≤–µ—Ä—Å–∏–∏)'
    )
    
    args = parser.parse_args()
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
    logger, log_filename = setup_logging()
    
    logger.info("="*80)
    logger.info("üöÄ BATCH IMPORT STARTED")
    logger.info("="*80)
    logger.info(f"Log directory: {args.log_dir}")
    logger.info(f"VM URL: {args.vm_url}")
    logger.info(f"Skip existing: {args.skip_existing}")
    logger.info(f"Dry-run: {args.dry_run}")
    logger.info("="*80)
    logger.info("")
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
    log_dir_path = Path(args.log_dir)
    if not log_dir_path.exists():
        logger.error(f"‚ùå –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {log_dir_path}")
        sys.exit(1)
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è VM –∫–ª–∏–µ–Ω—Ç–∞
    vm_client = None
    if VM_CLIENT_AVAILABLE:
        try:
            vm_client = VictoriaMetricsClient(vm_url=args.vm_url)
            if vm_client.check_availability():
                logger.info("‚úÖ VictoriaMetrics –¥–æ—Å—Ç—É–ø–Ω–∞")
            else:
                logger.warning("‚ö†Ô∏è  VictoriaMetrics –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞, –ø—Ä–æ–≤–µ—Ä–∫–∞ –¥–∞–Ω–Ω—ã—Ö –±—É–¥–µ—Ç –ø—Ä–æ–ø—É—â–µ–Ω–∞")
                vm_client = None
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è  –ù–µ —É–¥–∞–ª–æ—Å—å –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å VM –∫–ª–∏–µ–Ω—Ç: {e}")
            vm_client = None
    else:
        logger.warning("‚ö†Ô∏è  VictoriaMetricsClient –Ω–µ –¥–æ—Å—Ç—É–ø–µ–Ω, –ø—Ä–æ–≤–µ—Ä–∫–∞ –¥–∞–Ω–Ω—ã—Ö –±—É–¥–µ—Ç –ø—Ä–æ–ø—É—â–µ–Ω–∞")
    
    # –ü–æ–∏—Å–∫ –∞—Ä—Ö–∏–≤–æ–≤ (.zip –∏ .7z) - —Ä–µ–∫—É—Ä—Å–∏–≤–Ω–æ –≤–æ –≤—Å–µ—Ö –ø–æ–¥–¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è—Ö
    logger.info("üîç –ü–æ–∏—Å–∫ –∞—Ä—Ö–∏–≤–æ–≤ (.zip, .7z)...")
    
    # –†–µ–∫—É—Ä—Å–∏–≤–Ω—ã–π –ø–æ–∏—Å–∫ –≤—Å–µ—Ö –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤
    archive_files = []
    archive_files.extend(log_dir_path.rglob("*.zip"))
    archive_files.extend(log_dir_path.rglob("*.7z"))
    
    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –∏–º–µ–Ω–∏ –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑—É–µ–º–æ–≥–æ –ø–æ—Ä—è–¥–∫–∞
    archive_files = sorted(archive_files, key=lambda x: x.name)
    
    if not archive_files:
        logger.error(f"‚ùå –ê—Ä—Ö–∏–≤—ã (.zip, .7z) –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –≤ {log_dir_path}")
        sys.exit(1)
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Ç–∏–ø–∞–º –∞—Ä—Ö–∏–≤–æ–≤
    zip_count = sum(1 for f in archive_files if f.suffix.lower() == '.zip')
    sevenz_count = sum(1 for f in archive_files if f.suffix.lower() == '.7z')
    
    logger.info(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(archive_files)} –∞—Ä—Ö–∏–≤–æ–≤:")
    if zip_count > 0:
        logger.info(f"   - ZIP: {zip_count}")
    if sevenz_count > 0:
        logger.info(f"   - 7z:  {sevenz_count}")
        if not PY7ZR_AVAILABLE:
            logger.warning("‚ö†Ô∏è  py7zr –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω! .7z —Ñ–∞–π–ª—ã –±—É–¥—É—Ç –ø—Ä–æ–ø—É—â–µ–Ω—ã")
            logger.warning("   –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install py7zr")
            archive_files = [f for f in archive_files if f.suffix.lower() != '.7z']
    logger.info("")
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
    stats = BatchStats(total_archives=len(archive_files))
    start_time = time.time()
    
    # –û–±—Ä–∞–±–æ—Ç–∫–∞ –∞—Ä—Ö–∏–≤–æ–≤
    for idx, archive_file in enumerate(archive_files, 1):
        if INTERRUPTED:
            logger.warning("‚ö†Ô∏è  –ü—Ä–µ—Ä–≤–∞–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
            break
        
        logger.info(f"[{idx}/{len(archive_files)}] –û–±—Ä–∞–±–æ—Ç–∫–∞ {archive_file.name}...")
        
        try:
            result = process_archive(
                archive_file,
                args.vm_url,
                vm_client,
                args.skip_existing,
                args.dry_run,
                logger
            )
            
            stats.results.append(result)
            stats.processed += 1
            
            if result.status == "success":
                stats.successful += 1
            elif result.status == "failed":
                stats.failed += 1
            elif result.status == "skipped":
                stats.skipped += 1
            
        except Exception as e:
            logger.error(f"‚ùå –ù–µ–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ {archive_file.name}: {e}")
            result = ImportResult(
                archive_name=archive_file.name,
                status="failed",
                error_message=str(e)
            )
            stats.results.append(result)
            stats.processed += 1
            stats.failed += 1
        
        logger.info("")
    
    # –§–∏–Ω–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    stats.total_time = time.time() - start_time
    
    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–∞
    generate_report(stats, log_filename, logger)
    
    # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –∫–æ–¥ –≤—ã—Ö–æ–¥–∞
    if stats.failed > 0:
        sys.exit(1)
    else:
        sys.exit(0)


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n‚ö†Ô∏è  –ü—Ä–µ—Ä–≤–∞–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
        sys.exit(130)
    except Exception as e:
        print(f"\n‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

