#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –≤—Å–µ—Ö —Ä–µ—Å—É—Ä—Å–æ–≤ –≤ –∞—Ä—Ö–∏–≤–µ.
–ü–æ–∫–∞–∑—ã–≤–∞–µ—Ç, –∫–∞–∫–∏–µ Resource IDs —Ä–µ–∞–ª—å–Ω–æ –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É—é—Ç –≤ –¥–∞–Ω–Ω—ã—Ö.
"""

import struct
import json
import sys
import zipfile
import tarfile
import tempfile
from pathlib import Path
from collections import defaultdict

sys.path.insert(0, str(Path(__file__).parent / 'Data2csv'))
from Data2csv.RESOURCE_DICT import RESOURCE_NAME_DICT


def analyze_dat_file(file_path: Path):
    """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç .dat —Ñ–∞–π–ª –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö Resource IDs."""
    resources_found = set()
    
    try:
        with open(file_path, 'rb') as f:
            # –ß–∏—Ç–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ —Ñ–∞–π–ª–∞
            bit_correct = f.read(32)
            bit_msg_version = struct.unpack('<I', f.read(4))[0]
            bit_equip_sn = f.read(256).decode('utf-8', errors='ignore').strip('\x00')
            bit_equip_name = f.read(41).decode('utf-8', errors='ignore').strip('\x00')
            bit_equip_data_length = struct.unpack('<I', f.read(4))[0]
            
            # –ß–∏—Ç–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –±–ª–æ–∫–∏
            while f.tell() < len(bit_correct) + 4 + 256 + 41 + 4 + bit_equip_data_length:
                try:
                    bit_map_type = struct.unpack('<I', f.read(4))[0]
                    bit_map_length = struct.unpack('<I', f.read(4))[0]
                    bit_map_value = f.read(bit_map_length).decode('utf-8', errors='ignore')
                    
                    # –ü–∞—Ä—Å–∏–º JSON –∫–∞—Ä—Ç—É
                    try:
                        map_data = json.loads(bit_map_value)
                    except json.JSONDecodeError:
                        continue
                    
                    # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ Resource IDs –∏–∑ Map
                    resource_map = map_data.get('Map', {})
                    for resource_id in resource_map.keys():
                        resources_found.add(resource_id)
                    
                    # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –±–∏–Ω–∞—Ä–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ (–Ω–µ —á–∏—Ç–∞–µ–º –∏—Ö)
                    # –í—ã—á–∏—Å–ª—è–µ–º —Ä–∞–∑–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö
                    time_diff = int(map_data.get('EndTime', 0)) - int(map_data.get('StartTime', 0))
                    archive_interval = int(map_data.get('Archive', 60))
                    num_points = max(1, time_diff // archive_interval)
                    
                    for resource_id, resource_data in resource_map.items():
                        num_elements = len(resource_data.get('IDs', []))
                        num_metrics = len(resource_data.get('DataTypes', []))
                        data_size = num_points * num_elements * num_metrics * 4
                        f.read(data_size)
                    
                except struct.error:
                    break
                except Exception as e:
                    break
            
            return bit_equip_sn, resources_found
            
    except Exception as e:
        print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞: {e}")
        return None, set()


def main():
    if len(sys.argv) < 2:
        print("Usage: python3 analyze_archive_resources.py <path_to_zip_archive>")
        sys.exit(1)
    
    archive_path = Path(sys.argv[1])
    if not archive_path.exists():
        print(f"‚ùå –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {archive_path}")
        sys.exit(1)
    
    print(f"\n{'='*80}")
    print(f"üîç –ê–ù–ê–õ–ò–ó –ê–†–•–ò–í–ê: {archive_path.name}")
    print(f"{'='*80}\n")
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Ä–µ—Å—É—Ä—Å–∞–º
    all_resources = defaultdict(int)
    serial_numbers = set()
    files_checked = 0
    
    # –°–æ–∑–¥–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é
    with tempfile.TemporaryDirectory() as temp_dir:
        temp_path = Path(temp_dir)
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º ZIP
        print(f"üì¶ –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ ZIP –∞—Ä—Ö–∏–≤–∞...")
        with zipfile.ZipFile(archive_path, 'r') as zip_ref:
            zip_ref.extractall(temp_path)
        
        # –ù–∞—Ö–æ–¥–∏–º –≤—Å–µ .tgz —Ñ–∞–π–ª—ã
        tgz_files = list(temp_path.rglob("*.tgz"))
        print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(tgz_files)} .tgz —Ñ–∞–π–ª–æ–≤\n")
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –ø–µ—Ä–≤—ã–µ 20 —Ñ–∞–π–ª–æ–≤ –¥–ª—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        sample_size = min(20, len(tgz_files))
        print(f"üìä –ê–Ω–∞–ª–∏–∑ –ø–µ—Ä–≤—ã—Ö {sample_size} —Ñ–∞–π–ª–æ–≤...\n")
        
        for tgz_file in tgz_files[:sample_size]:
            try:
                # –ò–∑–≤–ª–µ–∫–∞–µ–º .dat –∏–∑ .tgz
                with tarfile.open(tgz_file, 'r:gz') as tar:
                    members = tar.getmembers()
                    if members:
                        tar.extract(members[0], temp_path / "extracted")
                        dat_file = temp_path / "extracted" / members[0].name
                        
                        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º
                        sn, resources = analyze_dat_file(dat_file)
                        if sn:
                            serial_numbers.add(sn)
                        
                        for resource_id in resources:
                            all_resources[resource_id] += 1
                        
                        files_checked += 1
                        
                        # Cleanup
                        dat_file.unlink()
                        
            except Exception as e:
                print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ {tgz_file.name}: {e}")
                continue
    
    # –í—ã–≤–æ–¥–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    print(f"\n{'='*80}")
    print(f"üìä –†–ï–ó–£–õ–¨–¢–ê–¢–´ –ê–ù–ê–õ–ò–ó–ê")
    print(f"{'='*80}\n")
    
    print(f"üìÅ –§–∞–π–ª–æ–≤ –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ: {files_checked}")
    print(f"üìå –°–µ—Ä–∏–π–Ω—ã–µ –Ω–æ–º–µ—Ä–∞: {', '.join(serial_numbers)}\n")
    
    print(f"üîç –ù–ê–ô–î–ï–ù–ù–´–ï –†–ï–°–£–†–°–´ –í –î–ê–ù–ù–´–•:")
    print(f"{'‚îÄ'*80}")
    print(f"{'Resource ID':<15} {'–ù–∞–∑–≤–∞–Ω–∏–µ':<40} {'–í—Å—Ç—Ä–µ—á–∞–µ—Ç—Å—è –≤ —Ñ–∞–π–ª–∞—Ö':<25}")
    print(f"{'‚îÄ'*80}")
    
    for resource_id in sorted(all_resources.keys(), key=lambda x: int(x) if x.isdigit() else 999999):
        resource_name = RESOURCE_NAME_DICT.get(resource_id, f"UNKNOWN_RESOURCE_{resource_id}")
        count = all_resources[resource_id]
        percentage = (count / files_checked * 100) if files_checked > 0 else 0
        
        status = "‚úÖ" if not resource_name.startswith("UNKNOWN") else "‚ùå"
        print(f"{status} {resource_id:<13} {resource_name:<40} {count}/{files_checked} ({percentage:.0f}%)")
    
    print(f"{'‚îÄ'*80}")
    print(f"\n–í—Å–µ–≥–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ä–µ—Å—É—Ä—Å–æ–≤: {len(all_resources)}")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º NFSv3 —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ
    print(f"\n{'='*80}")
    print(f"üîç –ü–†–û–í–ï–†–ö–ê NFSv3 –†–ï–°–£–†–°–ê")
    print(f"{'='*80}")
    
    nfsv3_id = "1000"
    if nfsv3_id in all_resources:
        print(f"‚úÖ NFSv3 —Ä–µ—Å—É—Ä—Å (ID: {nfsv3_id}) –ù–ê–ô–î–ï–ù!")
        print(f"   –í—Å—Ç—Ä–µ—á–∞–µ—Ç—Å—è –≤ {all_resources[nfsv3_id]}/{files_checked} —Ñ–∞–π–ª–∞—Ö")
    else:
        print(f"‚ùå NFSv3 —Ä–µ—Å—É—Ä—Å (ID: {nfsv3_id}) –ù–ï –ù–ê–ô–î–ï–ù –≤ –∞—Ä—Ö–∏–≤–µ!")
        print(f"\nüí° –í–æ–∑–º–æ–∂–Ω—ã–µ –ø—Ä–∏—á–∏–Ω—ã:")
        print(f"   1. NFSv3 –Ω–µ –±—ã–ª –Ω–∞—Å—Ç—Ä–æ–µ–Ω –Ω–∞ —ç—Ç–æ–º –º–∞—Å—Å–∏–≤–µ")
        print(f"   2. NFSv3 –º–µ—Ç—Ä–∏–∫–∏ –Ω–µ —Å–æ–±–∏—Ä–∞–ª–∏—Å—å –≤ –ø–µ—Ä–∏–æ–¥ —Å–±–æ—Ä–∞ –ª–æ–≥–æ–≤")
        print(f"   3. –ù–µ–ø—Ä–∞–≤–∏–ª—å–Ω—ã–π Resource ID (–º–æ–∂–µ—Ç –±—ã—Ç—å –¥—Ä—É–≥–æ–π ID)")
        print(f"\nüîç –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –¥—Ä—É–≥–∏–µ Controller —Ä–µ—Å—É—Ä—Å—ã:")
        for rid in sorted(all_resources.keys()):
            rname = RESOURCE_NAME_DICT.get(rid, "")
            if "Controller" in rname or "NFS" in rname.upper():
                print(f"   ‚Ä¢ ID {rid}: {rname}")
    
    print(f"{'='*80}\n")


if __name__ == "__main__":
    main()

