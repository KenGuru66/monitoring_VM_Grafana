#!/usr/bin/env python3
"""
–û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–ê–Ø –≤–µ—Ä—Å–∏—è –∏–º–ø–æ—Ä—Ç–∞ CSV –≤ VictoriaMetrics —Å –ø–æ—Ç–æ–∫–æ–≤–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π.

–û—Å–Ω–æ–≤–Ω—ã–µ —É–ª—É—á—à–µ–Ω–∏—è:
- –ü–æ—Ç–æ–∫–æ–≤–æ–µ —á—Ç–µ–Ω–∏–µ: —á–∏—Ç–∞–µ—Ç –∏ –æ—Ç–ø—Ä–∞–≤–ª—è–µ—Ç –¥–∞–Ω–Ω—ã–µ –ø–æ—Ä—Ü–∏—è–º–∏, –Ω–µ –∑–∞–≥—Ä—É–∂–∞—è –≤–µ—Å—å —Ñ–∞–π–ª –≤ –ø–∞–º—è—Ç—å
- –ú–µ–Ω—å—à–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ RAM: ~10-50 MB –≤–º–µ—Å—Ç–æ 2-3 GB
- –ë—ã—Å—Ç—Ä–µ–µ —Å—Ç–∞—Ä—Ç: –Ω–∞—á–∏–Ω–∞–µ—Ç –æ—Ç–ø—Ä–∞–≤–∫—É –¥–∞–Ω–Ω—ã—Ö —Å—Ä–∞–∑—É –ø–æ—Å–ª–µ —á—Ç–µ–Ω–∏—è –ø–µ—Ä–≤–æ–≥–æ –±–∞—Ç—á–∞
- –ü—Ä–æ–≥—Ä–µ—Å—Å –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏: –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç —Å–∫–æ—Ä–æ—Å—Ç—å –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å—Ç—Ä–æ–∫/—Å–µ–∫
"""

import argparse
import csv
import pathlib
import requests
import sys
import logging
import time
from datetime import datetime

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('csv2vm_streaming.log', mode='a', encoding='utf-8'),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)

# URL endpoint –¥–ª—è –∏–º–ø–æ—Ä—Ç–∞ –¥–∞–Ω–Ω—ã—Ö –≤ VictoriaMetrics
DEFAULT_URL = "http://localhost:8428/api/v1/import/prometheus"

def detect_delimiter(path: pathlib.Path) -> str:
    """–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å CSV —Ñ–∞–π–ª–∞."""
    with open(path, 'r', encoding='utf-8') as f:
        first_line = f.readline()
        if '\t' in first_line:
            return '\t'
        elif ';' in first_line:
            return ';'
        return ','

def sanitize_metric_name(name: str) -> str:
    """–ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç —Å—Ç—Ä–æ–∫—É –≤ –≤–∞–ª–∏–¥–Ω–æ–µ –∏–º—è –º–µ—Ç—Ä–∏–∫–∏ Prometheus."""
    result = name.strip().lower()
    result = result.replace(" (%)", "_pct")
    result = result.replace("(%)", "_pct")
    result = result.replace(" (mbps)", "_mbps")
    result = result.replace(" (iops)", "_iops")
    result = result.replace("(", "").replace(")", "")
    result = result.replace(" ", "_")
    result = result.replace("-", "_")
    result = result.replace("/", "_")
    result = result.replace("%", "percent")
    result = result.replace(".", "")
    result = result.replace(",", "")
    result = result.replace(":", "")
    result = result.replace("[", "").replace("]", "")
    result = result.replace("+", "plus")
    result = result.replace("‚àû", "inf")
    return result

def row_to_prom(row: list, array_sn: str) -> str:
    """–ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç —Å—Ç—Ä–æ–∫—É CSV –≤ —Ñ–æ—Ä–º–∞—Ç Prometheus."""
    if len(row) < 6:
        return None
    
    try:
        resource = row[0].strip()
        metric_name = "hu_" + sanitize_metric_name(row[1]) + "_variable"
        element = row[2].strip()
        value = row[3].strip()
        ts_unix_sec = float(row[5].strip())
        ts_unix_ms = int(ts_unix_sec * 1000)
        
        labels = {
            "Element": element,
            "Resource": resource,
            "SN": array_sn
        }
        
        label_str = ",".join(f'{k}="{v}"' for k, v in sorted(labels.items()))
        return f"{metric_name}{{{label_str}}} {value} {ts_unix_ms}\n"
    except (ValueError, IndexError) as e:
        logger.debug(f"Error processing row: {e}")
        return None

def send_batch(url: str, batch_lines: list, batch_num: int) -> bool:
    """–û—Ç–ø—Ä–∞–≤–ª—è–µ—Ç –±–∞—Ç—á –¥–∞–Ω–Ω—ã—Ö –≤ VictoriaMetrics."""
    if not batch_lines:
        return True
    
    payload = "".join(batch_lines).encode('utf-8')
    
    try:
        r = requests.post(url, data=payload, timeout=60)
        if r.status_code not in (200, 204):
            logger.error(f"Batch {batch_num}: HTTP {r.status_code} - {r.text[:200]}")
            return False
        return True
    except requests.RequestException as e:
        logger.error(f"Batch {batch_num}: {e}")
        return False

def main_streaming(path: pathlib.Path, url: str, batch: int):
    """
    –ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è —Å –ø–æ—Ç–æ–∫–æ–≤–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π.
    
    –ß–∏—Ç–∞–µ—Ç CSV –ø–æ—Ä—Ü–∏—è–º–∏ –∏ —Å—Ä–∞–∑—É –æ—Ç–ø—Ä–∞–≤–ª—è–µ—Ç –≤ VictoriaMetrics,
    –Ω–µ –∑–∞–≥—Ä—É–∂–∞—è –≤–µ—Å—å —Ñ–∞–π–ª –≤ –ø–∞–º—è—Ç—å.
    """
    logger.info("="*80)
    logger.info(f"CSV2VM Streaming Import Started at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    logger.info("="*80)
    logger.info(f"üöÄ Starting STREAMING import from: {path}")
    logger.info(f"üìç VictoriaMetrics URL: {url}")
    logger.info(f"üì¶ Batch size: {batch:,} lines")
    
    array_sn = path.stem
    logger.info(f"üî¢ Array SN: {array_sn}")
    
    delimiter = detect_delimiter(path)
    logger.info(f"üìã Detected delimiter: {'TAB' if delimiter == chr(9) else repr(delimiter)}")
    
    # –ü–æ—Ç–æ–∫–æ–≤–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞
    logger.info(f"üìñ Starting streaming processing...")
    
    batch_lines = []
    batch_num = 0
    total_rows_processed = 0
    total_lines_converted = 0
    skipped_rows = 0
    start_time = time.time()
    last_log_time = start_time
    
    with open(path, 'r', encoding='utf-8') as f:
        reader = csv.reader(f, delimiter=delimiter)
        
        for idx, row in enumerate(reader):
            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å—Ç—Ä–æ–∫–∏ —Å –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω—ã–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –∫–æ–ª–æ–Ω–æ–∫
            if len(row) < 6:
                skipped_rows += 1
                continue
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫
            if idx == 0:
                try:
                    float(row[3])
                except (ValueError, IndexError):
                    logger.info(f"   Skipping header row")
                    skipped_rows += 1
                    continue
            
            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Å—Ç—Ä–æ–∫—É
            prom_line = row_to_prom(row, array_sn)
            if prom_line:
                batch_lines.append(prom_line)
                total_lines_converted += 1
            
            total_rows_processed += 1
            
            # –ö–æ–≥–¥–∞ –Ω–∞–∫–æ–ø–∏–ª–∏ –±–∞—Ç—á - –æ—Ç–ø—Ä–∞–≤–ª—è–µ–º
            if len(batch_lines) >= batch:
                batch_num += 1
                success = send_batch(url, batch_lines, batch_num)
                
                if not success:
                    logger.error(f"Failed to send batch {batch_num}, aborting")
                    sys.exit(1)
                
                # –õ–æ–≥–∏—Ä—É–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å –∫–∞–∂–¥—ã–µ 10 –±–∞—Ç—á–µ–π –∏–ª–∏ –∫–∞–∂–¥—ã–µ 30 —Å–µ–∫—É–Ω–¥
                current_time = time.time()
                if batch_num % 10 == 0 or (current_time - last_log_time) >= 30:
                    elapsed = current_time - start_time
                    rate = total_rows_processed / elapsed if elapsed > 0 else 0
                    logger.info(
                        f"   üìä Batch {batch_num}: {total_rows_processed:,} rows processed, "
                        f"{total_lines_converted:,} lines sent, "
                        f"Rate: {rate:,.0f} rows/sec"
                    )
                    last_log_time = current_time
                
                # –û—á–∏—â–∞–µ–º –±–∞—Ç—á
                batch_lines = []
    
    # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –æ—Å—Ç–∞—Ç–æ–∫
    if batch_lines:
        batch_num += 1
        logger.info(f"   Sending final batch {batch_num} with {len(batch_lines):,} lines...")
        success = send_batch(url, batch_lines, batch_num)
        if not success:
            logger.error(f"Failed to send final batch")
            sys.exit(1)
    
    # –ò—Ç–æ–≥–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    total_time = time.time() - start_time
    avg_rate = total_rows_processed / total_time if total_time > 0 else 0
    
    logger.info("="*80)
    logger.info(f"‚úÖ Import completed successfully!")
    logger.info(f"üìä Statistics:")
    logger.info(f"   - Total rows processed: {total_rows_processed:,}")
    logger.info(f"   - Lines converted: {total_lines_converted:,}")
    logger.info(f"   - Rows skipped: {skipped_rows:,}")
    logger.info(f"   - Batches sent: {batch_num}")
    logger.info(f"   - Total time: {total_time:.1f} seconds ({total_time/60:.1f} minutes)")
    logger.info(f"   - Average rate: {avg_rate:,.0f} rows/sec")
    logger.info(f"   - Array SN: {array_sn}")
    logger.info("="*80)
    
    print(f"‚úÖ Imported {total_lines_converted:,} rows for SN={array_sn} in {total_time:.1f}s")

if __name__ == "__main__":
    p = argparse.ArgumentParser(
        description="STREAMING Import OceanStor CSV/TSV to VictoriaMetrics (Optimized)",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è —Å –ø–æ—Ç–æ–∫–æ–≤–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π:
- –ù–µ –∑–∞–≥—Ä—É–∂–∞–µ—Ç –≤–µ—Å—å —Ñ–∞–π–ª –≤ –ø–∞–º—è—Ç—å
- –ù–∞—á–∏–Ω–∞–µ—Ç –æ—Ç–ø—Ä–∞–≤–∫—É –¥–∞–Ω–Ω—ã—Ö —Å—Ä–∞–∑—É
- –ü–æ–∫–∞–∑—ã–≤–∞–µ—Ç –ø—Ä–æ–≥—Ä–µ—Å—Å –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏
- –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –≤ 100 —Ä–∞–∑ –º–µ–Ω—å—à–µ RAM

–ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:
  %(prog)s 2102353TJWFSP3100020.csv
  %(prog)s 2102353TJWFSP3100020.csv --batch 50000
  %(prog)s data.csv --url http://victoriametrics:8428/api/v1/import/prometheus
        """)
    
    p.add_argument("file", type=pathlib.Path, help="CSV/TSV file to import")
    p.add_argument("--url", default=DEFAULT_URL, 
                   help=f"VictoriaMetrics import endpoint (default: {DEFAULT_URL})")
    p.add_argument("--batch", type=int, default=50000, 
                   help="lines per batch request (default: 50000)")
    
    args = p.parse_args()
    
    if not args.file.exists():
        logger.error(f"‚ùå File not found: {args.file}")
        sys.exit(f"‚ùå File not found: {args.file}")
    
    try:
        main_streaming(args.file, args.url, args.batch)
        logger.info(f"Script completed at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    except KeyboardInterrupt:
        logger.warning("‚ö†Ô∏è Import interrupted by user")
        sys.exit(1)
    except Exception as e:
        logger.error(f"‚ùå Fatal error: {e}", exc_info=True)
        raise


